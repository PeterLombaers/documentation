---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Embedding"
redirect_from:
- /documentation/embedding.html
---

<p>A common technique in modern big data serving applications is to map the subject data - say, text or images -
to points in an abstract vector space and then do computation in that vector space. For example, retrieve
similar data by <a href="approximate-nn-hnsw.html">finding nearby points in the vector space</a>, or
<a href="onnx.html">using the vectors as input to a neural net</a>. This mapping is usually referred to as
<i>embedding</i>.</p>

<p>Vespa provides built-in support for embedding, which is documented here.</p>

<h2 id="embedders">Embedders</h2>

<p>Vespa provides a Java interface for defining components which can provide embeddings of text:
<a href="https://github.com/vespa-engine/vespa/blob/master/linguistics/src/main/java/com/yahoo/language/process/Embedder.java">
com.yahoo.language.process.Embedder</a>.</p>

<p>To define a custom embedder in an application and make it usable by Vespa (see <a href="#embedding-a-query-text">below</a>),
implement this interface and add it as a <a href="developer-guide.html#developing-components">component</a> to
<a href="reference/services-container.html">services.xml</a>:</p>

<pre>
&lt;container version="1.0"&gt;
    <span class="pre-hilite">&lt;component id="myEmbedder"</span>
               <span class="pre-hilite">class="com.example.MyEmbedder"</span>
               <span class="pre-hilite">bundle="the name in &lt;artifactId&gt; in pom.xml"/&gt;</span>
&lt;/container&gt;
</pre>



<h2 id="provided-embedders">Provided embedders</h2>

<p>Vespa provides some embedders as part of the platform.</p>


<h3 id="bertbase-embedder">BertBase embedder</h3>

<p>An embedder using <a href="#wordpiece-embedder">WordPiece</a> to produce tokens which is then input to
a supplied ONNX model on the form expected by a BERT base model. 
See <a href="https://github.com/vespa-engine/sample-apps/blob/master/simple-semantic-search/export_model_from_hf.py">export_model_from_hf.py</a>,
for how to export a <a href="https://www.sbert.net/">sentence-transformer</a> model to ONNX format.
See also <a href="#troubleshooting-model-signature">troubleshooting model signature</a>.
</p>

<p>This provides embeddings directly suitable for retrieval and ranking in Vespa, and makes it possible
to implement semantic search with no need for custom components or client-side embedding
when used with the syntax for invoking the embedder in queries and during indexing described
<a href="#embedding-a-query-text">below</a>.</p>

<p>To set up the
<a href="https://github.com/vespa-engine/vespa/blob/master/model-integration/src/main/java/ai/vespa/embedding/BertBaseEmbedder.java">
BertBase embedder</a>, add it to services.xml:</p>

<pre>
&lt;component id="myBert"
           class=&quot;ai.vespa.embedding.BertBaseEmbedder&quot;
           bundle=&quot;model-integration&quot;&gt;
    &lt;config name=&quot;embedding.bert-base-embedder&quot;&gt;
        &lt;transformerModel path="models/myBertModel.onnx"/&gt;
        &lt;tokenizerVocab path="models/myTokenizerVocabulary.txt"/&gt;
    &lt;/config&gt;
&lt;/component&gt;
</pre>

<p>See the options available for configuring the BertBase embedder in
<a href="https://github.com/vespa-engine/vespa/blob/master/model-integration/src/main/resources/configdefinitions/embedding.bert-base-embedder.def">
the full configuration definition</a>. Notice that BertBase embedder uses mean pooling strategy by default. 
</p>

<p>The model files used must be supplied by the application (or specified by id when on Vespa Cloud),
for example from <a href="https://huggingface.co/models?sort=downloads&search=bert-base">HuggingFace</a>.
Refer to <a href="configuring-components.html#adding-files-to-the-component-configuration">adding files to the configuration</a>
for the full syntax for specifying model files by url, path or model-id.</p>



<h3 id="sentencepiece-embedder">SentencePiece embedder</h3>

<p>A native Java implementation of <a href="https://github.com/google/sentencepiece">SentencePiece</a>.
SentencePiece breaks text into chunks independent of spaces, which is robust to misspellings and works with CJK languages.
It is also very fast.</p>

<p>This is suitable to use in conjunction with custom components which processes the resulting encoding further
to produce semantically meaningful vectors.</p>

<p>To use the
<a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/java/com/yahoo/language/sentencepiece/SentencePieceEmbedder.java">
SentencePiece embedder</a>, add it to services.xml:</p>

<pre>
&lt;component id="mySentencePiece"
           class=&quot;com.yahoo.language.sentencepiece.SentencePieceEmbedder&quot;
           bundle=&quot;linguistics-components&quot;&gt;
    &lt;config name=&quot;language.sentencepiece.sentence-piece&quot;&gt;
        &lt;model&gt;
            &lt;item&gt;
                &lt;language&gt;unknown&lt;/language&gt;
                &lt;path&gt;model/en.wiki.bpe.vs10000.model&lt;/path&gt;
            &lt;/item&gt;
        &lt;/model&gt;
    &lt;/config&gt;
&lt;/component&gt;
</pre>

<p>See the options available for configuring SentencePiece in
<a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/resources/configdefinitions/language.sentencepiece.sentence-piece.def">
the full configuration definition</a>.
</p>



<h3 id="wordpiece-embedder">WordPiece embedder</h3>

<p>A native Java implementation of
<a href="https://github.com/google-research/bert#tokenization">WordPiece</a>,
which is commonly used with BERT models.</p>

<p>This is suitable to use in conjunction with custom components which processes the resulting encoding further
to produce semantically meaningful vectors.</p>

<p>To use the
<a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/java/com/yahoo/language/wordpiece/WordPieceEmbedder.java">
  WordPiece embedder</a>, add it to services.xml:</p>

<pre>
&lt;component id="myWordPiece"
           class="com.yahoo.language.wordpiece.WordPieceEmbedder"
           bundle="linguistics-components"&gt;
    &lt;config name=&quot;language.wordpiece.word-piece&quot;&gt;
        &lt;model&gt;
            &lt;item&gt;
                &lt;language&gt;unknown&lt;/language&gt;
                &lt;path&gt;models/bert-base-uncased-vocab.txt&lt;/path&gt;
            &lt;/item&gt;
        &lt;/model&gt;
    &lt;/config&gt;
&lt;/component&gt;
</pre>

<p>See the options available for configuring WordPiece in
<a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/resources/configdefinitions/language.wordpiece.word-piece.def">
the full configuration definition</a>.
</p>

<p>WordPiece is suitable to use in conjunction with custom components which processes the resulting encoding further
to produce semantically meaningful vectors.</p>


<h2 id="embedding-a-query-text">Embedding a query text</h2>

<p>Where you would otherwise supply a tensor representing the vector point in a query,
you can with an embedder configured instead supply any text enclosed in <code>embed()</code>, e.g:</p>

<pre>
ranking.features.query(myEmbedding)=<span class="pre-hilite">embed(myEmbedderId, "Hello%20world")</span>
</pre>

<p>If you have only configured a single embedder, you can skip the embedder id argument and optionally also the quotes.
Both single and double quotes are permitted.</p>

<h2 id="embedding-a-document-field">Embedding a document field</h2>

<p>Use the indexing language to convert a text field into an embedding by using the <code>embed</code> function,
for example:</p>

<pre>
schema doc {

    document doc {

        field myText type string {
            indexing: index | summary
        }

    }

    field embeddingOfMyText type tensor(x[5]) {
        indexing: input myText | <span class="pre-hilite">embed myEmbedderId</span> | attribute | index | summary
        index: hnsw
    }

}
</pre>

<p>If each document has multiple text segments, represent them in an array and store the produced vector embeddings
in a tensor field with one mapped and one indexed dimension.
The array indexes (0-based) are used as labels in the mapped tensor dimension.
</p>

<pre>
schema doc {

    document doc {

        field myTextSegments type array&lt;string&gt; {
            indexing: index | summary
        }

    }

    field embeddingsOfMyTextSegments type tensor(s{},x[5]) {
        indexing: input myTextSegments | <span class="pre-hilite">embed myEmbedderId</span> | attribute | index | summary
        index: hnsw
    }

}
</pre>


<p>If you only have configured a single embedder you can skip the embedder id argument.</p>

{% include important.html content='Note that the generated embedding field
is defined outside the <code>document</code> clause in the schema.' %}


<h2 id="using-an-embedder-from-java">Using an embedder from Java</h2>

<p>
When writing custom Java components (such as <a href="searcher-development.html">Searchers</a>
or <a href="document-processing.html#document-processors">Document processors</a>),
use embedders you have configured by
<a href="jdisc/injecting-components.html">having them injected in the constructor</a>,
just as any other component:
</p>
<pre>
class MyComponent(ComponentRegistry&lt;Embedder&gt; embedders) {
    // embedders contains all the embedders configured in your services.xml
}
</pre>



<h2 id="examples">Examples</h2>
<p>
  Try the <a href="https://github.com/vespa-engine/sample-apps/tree/master/simple-semantic-search">
  simple-semantic-search</a> sample application.
  A complete example application using multiple embedders can be found in
  in <a href="https://github.com/vespa-engine/system-test/blob/master/tests/search/embedding/app_two_embedders/">this system test</a>.
</p>



<h2 id="troubleshooting-model-signature">Troubleshooting model signature</h2>
<p>
  When loading models for the embedder, the model must have correct inputs and output signatures.
  Here, <em>minilm-l6-v2.onnx</em> is in current working directory:
</p>
<pre>
$ docker run -v `pwd`:/w \
  --entrypoint /opt/vespa/bin/vespa-analyze-onnx-model \
  vespaengine/vespa \
  /w/minilm-l6-v2.onnx

...
model meta-data:
  input[0]: 'input_ids' long[batch][sequence]
  input[1]: 'attention_mask' long[batch][sequence]
  input[2]: 'token_type_ids' long[batch][sequence]
  output[0]: 'output_0' float[batch][sequence][384]
  output[1]: 'output_1' float[batch][384]
...
test setup:
  input[0]: tensor(d0[1],d1[1]) -> long[1][1]
  input[1]: tensor(d0[1],d1[1]) -> long[1][1]
  input[2]: tensor(d0[1],d1[1]) -> long[1][1]
  output[0]: float[1][1][384] -> tensor&lt;float&gt;(d0[1],d1[1],d2[384])
  output[1]: float[1][384] -> tensor&lt;float&gt;(d0[1],d1[384])
</pre>
<p>
  If loading models with other signatures, the Vespa Container node will not start
  (check <em>vespa.log</em> in the container running Vespa):
</p>
<pre>
[2022-10-18 18:18:31.761] WARNING container        Container.com.yahoo.container.di.Container
  Failed to set up first component graph due to error when constructing one of the components
  exception=com.yahoo.container.di.componentgraph.core.ComponentNode$ComponentConstructorException:
  Error constructing 'bert' of type 'ai.vespa.embedding.BertBaseEmbedder': null
  Caused by: java.lang.IllegalArgumentException: Model does not contain required input: 'input_ids'. Model contains: input
  at ai.vespa.embedding.BertBaseEmbedder.validateName(BertBaseEmbedder.java:79)
  at ai.vespa.embedding.BertBaseEmbedder.validateModel(BertBaseEmbedder.java:68)
</pre>
<p>
  When this happens, a deploy looks like:
</p>
<pre>
$ vespa deploy --wait 300
Uploading application package ... done

Success: Deployed .

Waiting up to 5m0s for query service to become available ...
Error: service 'query' is unavailable: services have not converged
</pre>
<p>
  Use <a href="onnx.html#using-vespa-analyze-onnx-model">vespa-analyze-onnx-model</a> like in the example above
  to analyze the signature.
</p>
