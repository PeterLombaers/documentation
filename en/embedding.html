---
# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Embedding"
redirect_from:
- /documentation/embedding.html
---

<p>
  A common technique in modern big data serving applications is to map unstructured data - say, text or images -
  to points in an abstract vector space and then do computation in that vector space. For example, retrieve
  similar data by <a href="approximate-nn-hnsw.html">finding nearby points in the vector space</a>,
  or <a href="onnx.html">using the vectors as input to a neural net</a>.
  This mapping is usually referred to as <em>embedding</em>.
  Read more about embedding and embedding management in this
  <a href="https://blog.vespa.ai/tailoring-frozen-embeddings-with-vespa/">blog post</a>.
</p>

<p>Without using Vespa native embedding support, the embedding vectors must be provided by the client like below:</p>
<img src="/assets/img/vespa-overview-embeddings-1.svg" alt="document- and query-embeddings"
  width="890px" height="auto"/>

<p>
  By using the embedding features in Vespa, developers can instead generate the embeddings
  into the Vespa document and query flows.
  Observe how <code>embed</code> is used to create the vector embeddings from text in the schema and queries:
</p>
<img src="/assets/img/vespa-overview-embeddings-2.svg" alt="Vespa's embedding feature, creating embeddings from text"
  width="890px" height="auto"/>
<p>
  Integrating embedding <em>into</em> the Vespa application simplify the serving architecture,
  as well as cut serving latencies significantly - vector data transfer is minimized, with fewer components and serialization overhead.
</p>

<h2 id="provided-embedders">Provided embedders</h2>
<p>
  Vespa provides several embedders as part of the platform.
</p>
<p>An example of a Vespa <a href="query-api.html#http">query</a> request using Vespa embed functionality:</p>
<pre>{% highlight json %}
 {
    "yql": "select * from doc where {targetHits:10)nearestNeighbor(embedding,query_embedding)",
    "query": "semantic search",
    "input.query(query_embedding)": "embed(semantic search)",
}
{% endhighlight %}</pre>
<p>
  See more usage examples in <a href="#embedding-a-query-text">embedding a query text</a>
  and <a href="#embedding-a-document-field">embedding a document field</a>.
</p>

<h3 id="huggingface-embedder">Huggingface Embedder</h3>
<p>
  An embedder using any <a href="https://huggingface.co/docs/tokenizers/index">Huggingface tokenizer</a>,
  including multilingual tokenizers,
  to produce tokens which is then input to a supplied transformer model in <a href="https://onnx.ai/">ONNX</a> model format.
</p>
<p>The huggingface-embedder supports all <a href="https://huggingface.co/docs/tokenizers/index">Huggingface tokenizer implementations</a>
 and input length settings, while the <a href="#bert-embedder">bert-embedder</a>
 only supports <a href="reference/embedding-reference.html#wordpiece-embedder">WordPiece</a> tokenization.</p>
<p>
  The Huggingface Embedder provides embeddings directly suitable for <a href="approximate-nn-hnsw.html">retrieval</a>
  and <a href="ranking.html">ranking</a> in Vespa, and makes it easy
  to implement semantic search with no need for custom components or client-side embedding inference
  when used with the syntax for invoking the embedder in queries and during document indexing described
  in <a href="#embedding-a-query-text">embedding a query text</a> and <a href="#embedding-a-document-field">embedding a document field</a>.
</p>
<p>
  The Huggingface embedder is configured in <a href="reference/services.html">services.xml</a>,
  within the <code>container</code> tag:
</p>
<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="hf-embedder" type="hugging-face-embedder">
        <transformer-model path="my-models/model.onnx"/>
        <tokenizer-model path="my-models/tokenizer.json"/>
    </component>
    ...
</container>
{% endhighlight %}</pre>
<ul>
  <li>
    The <code>transformer-model</code> specifies the embedding model in <a href="https://onnx.ai/">ONNX</a>.
    See <a href="#onnx-export">exporting models to ONNX</a>,
    for how to export embedding models from Huggingface to compatible <a href="https://onnx.ai/">ONNX</a> format.
  </li>
  <li>
    The <code>tokenizer-model</code> specifies the Huggingface <code>tokenizer.json</code> formatted file.
    See <a href="https://huggingface.co/transformers/v4.8.0/fast_tokenizers.html#loading-from-a-json-file">HF loading tokenizer from a json file.</a>
  </li>
</ul>
<p>
  When using <code>path</code>, the model files must be supplied in the Vespa
  <a href="application-packages.html#deploying-remote-models">application package</a>.
  The above example uses files in the the <code>models</code> directory,
  or specified by <code>model-id</code> when deployed on <a href="https://cloud.vespa.ai/en/model-hub">Vespa Cloud</a>.
  See <a href="reference/embedding-reference.html#model-config-reference">model config reference</a>.
</p>
<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="e5" type="hugging-face-embedder">
        <transformer-model path="my-models/model.onnx"/>
        <tokenizer-model url="https://huggingface.co/intfloat/e5-base-v2/raw/main/tokenizer.json"/>
    </component>
    ...
</container>{% endhighlight %}</pre>
<p>See <a href="reference/embedding-reference.html#huggingface-embedder-reference-config">configuration reference</a> for all the parameters.</p>

<h4 id="huggingface-embedder-models">Huggingface embedder models</h4>
<p>
  The following are examples of text embedding models that can be used with the hugging-face-embedder
  and their output <a href="tensor-user-guide.html">tensor</a> dimensionality.
  The resulting <a href="reference/tensor.html#tensor-type-spec">tensor type</a> can be either <code>float</code>
  or <code>bfloat16</code>:
</p>
  <ul>
    <li><a href="https://huggingface.co/intfloat/e5-small-v2">intfloat/e5-small-v2</a> produces <code>tensor&lt;float&gt;(x[384])</code></li>
    <li><a href="https://huggingface.co/intfloat/e5-base-v2">intfloat/e5-base-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/intfloat/e5-large-v2">intfloat/e5-large-v2 produces <code>tensor&lt;float&gt;(x[1024])</code></a></li>
    <li><a href="https://huggingface.co/intfloat/multilingual-e5-base">intfloat/multilingual-e5-base</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">sentence-transformers/all-MiniLM-L6-v2</a> produces <code>tensor&lt;float&gt;(x[384])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2">sentence-transformers/all-mpnet-base-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2">sentence-transformers/paraphrase-multilingual-mpnet-base-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
  </ul>
<p>
  All of these example text embedding models can be used in combination with Vespa's
  <a href="nearest-neighbor-search.html">nearest neighbor search</a>
  using <a href="reference/schema-reference.html#distance-metric">distance-metric</a> <code>angular</code>.
</p>
<p>
  Check the <a href="https://huggingface.co/blog/mteb">Massive Text Embedding Benchmark</a> (MTEB) benchmark and
  <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB leaderboard</a>
  for help with choosing an embedding model.
</p>


<h3 id="bert-embedder">Bert embedder</h3>
<p>
  An embedder using the <a href="reference/embedding-reference.html#wordpiece-embedder">WordPiece</a> embedder to produce tokens
  which is then input to a supplied <a href="https://onnx.ai/">ONNX</a> model on the form expected by a BERT base model.
  The Bert embedder is limited to English (<a href="reference/embedding-reference.html#wordpiece-embedder">WordPiece</a>) and
  BERT-styled transformer models with three model inputs
  (<em>input_ids, attention_mask, token_type_ids</em>).
  Prefer using the <a href="#huggingface-embedder">Huggingface Embedder</a> instead of the Bert embedder.
</p>
<p>
  The Bert embedder is configured in <a href="reference/services.html">services.xml</a>,
  within the <code>container</code> tag:
</p>
<pre>{% highlight xml %}
<container version="1.0">
  <component id="myBert" type="bert-embedder">
    <transformer-model path="models/e5-small-v2.onnx"/>
    <tokenizer-vocab url="https://huggingface.co/intfloat/e5-small-v2/raw/main/vocab.txt"/>
    <max-tokens>128</max-tokens>
  </component>
</container>
{% endhighlight %}</pre>
<ul>
  <li>
    The <code>transformer-model</code> specifies the embedding model in <a href="https://onnx.ai/">ONNX</a>.
    See <a href="#onnx-export">exporting models to ONNX</a>,
    for how to export embedding models from Huggingface to compatible <a href="https://onnx.ai/">ONNX</a> format.
  </li>
  <li>
    The <code>tokenizer-vocab</code> specifies the Huggingface <code>vocab.txt</code> file, with one valid token per line.
    Note that the Bert embedder does not support the <code>tokenizer.json</code> formatted tokenizer configuration files.
    This means that tokenization settings like max tokens should be set explicitly.
  </li>
</ul>
<p>See <a href="reference/embedding-reference.html#bert-embedder-reference-config">configuration reference</a> for all configuration options. </p>

<h2 id="embedding-a-query-text">Embedding a query text</h2>
<p>Where you would otherwise supply a tensor representing the vector point in a query,
you can with an embedder configured instead supply any text enclosed in <code>embed()</code>, e.g:</p>

<pre>
input.query(q)=<span class="pre-hilite">embed(myEmbedderId, "Hello%20world")</span>
</pre>
<p>If you have only configured a single embedder, you can skip the embedder id argument and optionally also the quotes. Prefer
  to specify the embedder id as introducing more embedder models requires specifying the identifier.
</p>
<p>Both single(') and double quotes(") are permitted. </p>

<p>Note that query <a href="reference/schema-reference.html#inputs">input tensors</a>
must be defined in the schema's rank-profile. See <a href="reference/schema-reference.html#inputs">schema reference inputs</a>.</p>
<pre>
inputs {
  query(q) tensor&lt;float&gt;(x[768])
  query(q2) tensor&lt;float&gt;(x[768])
}
</pre>
<p>Output from <code>embed</code> that cannot fit into the tensor dimensionality is truncated, only retaining the first values.</p>

<p>A single Vespa <a href="query-api.html#http">query</a> can use multiple embedders or embed multiple texts with the same embedder:</p>
<pre>{% highlight json %}
 {
    "yql": "select id,title from paragraph where ({targetHits:10}nearestNeighbor(embedding,q)) or ({targetHits:10}nearestNeighbor(embedding,q2)) or userQuery()",
    "query": "semantic search",
    "input.query(q)": "embed(e5, \"contextualized search\")",
    "input.query(q2)": "embed(e5, \"neural search\")"
    "ranking": "semantic",
}{% endhighlight %}</pre>
<p>Above example using <a href="query-api.html#using-post">JSON POST</a> query. Notice how the input to the embedding is quoted. Since
adding new embedders to services.xml will break queries not specifying embedder id, it is recommended to always specify embedder id.</p>

<pre>{% highlight json %}
  {
     "yql": "select id,title from paragraph where ({targetHits:10}nearestNeighbor(embedding,q)) or ({targetHits:10}nearestNeighbor(question_embedding,q))",
     "query": "semantic search",
     "input.query(q)": "embed(e5, \"contextualized search\")",
     "ranking": "semantic",
 }{% endhighlight %}</pre>
 <p>Using the same embedding tensor as input to two nearestNeighbor query operators, searching two different embedding fields. For this to
  work, both <em>embedding</em> and <em>question_embedding</em> must have the same dimensionality.
 </p>


<h2 id="embedding-a-document-field">Embedding a document field</h2>
<p>Use the Vespa <a href="reference/indexing-language-reference.html#statement">indexing language</a>
to convert one or more string fields into an embedding vector by using the <code>embed</code> function,
for example:</p>

<pre>
schema doc {

    document doc {

        field title type string {
            indexing: summary | index
        }

        field body type string {
            indexing: summary | index
        }

    }

    field embeddings type tensor&lt;bfloat16&gt;(x[384]) {
        indexing {
            (input title || "") . " " . (input body || "") | <span class="pre-hilite">embed embedderId</span> | attribute | index
        }
        index: hnsw
    }

}
</pre>
<p>
  The above example uses two input fields and concatenate them into a single input string to the embedder.
  See <a href="/en/indexing.html#choice-example">indexing choice</a> for details.
</p>

<p>If each document has multiple text segments, represent them in an array and store the vector embeddings
in a tensor field with one mapped and one indexed dimension.
The array indexes (0-based) are used as labels in the mapped tensor dimension.
See <a href="https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/">Revolutionizing Semantic Search with Multi-Vector HNSW Indexing in Vespa</a>.
</p>

<pre>
schema doc {

    document doc {

        field chunks type array&lt;string&gt; {
            indexing: index | summary
        }

    }

    field embeddings type tensor&lt;bfloat16&gt;(p{},x[5]) {
        indexing: input chunks | <span class="pre-hilite">embed embedderId</span> | attribute | index
        index: hnsw
    }

}
</pre>
<p>If you only have configured a single embedder you can skip the embedder id argument.</p>

<p>
  The indexing expression can also use <code>for_each</code> and include other document fields.
  For example the <em>E5</em> family of embedding models uses instructions along with the input. The following
  expression prefixes the input with <em>passage: </em> followed by a concatenation of the title and a text chunk.</p>

<pre>
schema doc {

    document doc {

        field title type string {
            indexing: summary | index
        }

        field chunks type array&lt;string&gt; {
            indexing: index | summary
        }

    }
    field embedding type tensor&lt;bfloat16&gt;(p{}, x[384]) {
        indexing {
            input chunks |
                for_each {
                    "passage: " . (input title || "") . " " . ( _ || "")
                } | embed e5 | attribute | index
        }
        attribute {
            distance-metric: prenormalized-angular
        }
    }
}
</pre>
<p>
  See <a href="/en/indexing.html#execution-value-example">Indexing language execution value</a>
  for details.
</p>

{% include important.html content='Note that the generated embedding fields
are defined outside the <code>document</code> clause in the schema.' %}


<h2 id="examples">Examples</h2>
<p>
  Try the <a href="https://github.com/vespa-engine/sample-apps/tree/master/simple-semantic-search">
  simple-semantic-search</a> sample application.
  The <a href="https://github.com/vespa-engine/sample-apps/tree/master/commerce-product-ranking">commerce-product-ranking</a>
  demonstrates using multiple embedders. The
  <a href="https://github.com/vespa-engine/sample-apps/tree/master/multi-vector-indexing">multi-vector-indexing</a> sample-app demonstrates
  how to use embedders with multiple document field inputs.
</p>

<h2 id="onnx-export">Exporting HF models to ONNX format</h2>
<p>Transformer based models have named inputs and outputs that needs to be compatible
  with the input and output names used by the Bert embedder or the Huggingface embedder.
</p>
<p>The <a href="https://github.com/vespa-engine/sample-apps/tree/master/simple-semantic-search#model-exporting">simple-semantic-search</a>
  sample app includes two scripts to export models and vocabulary files using the default expecteded input and output names for the bert-embedder
  and the huggingface-embedder. The input and output names to use can also be overriden by the various <code>transformer-</code>
  input and output <a href="reference/embedding-reference.html#huggingface-embedder-reference-config">huggingface-embedder parameters</a>
  and <a href="reference/embedding-reference.html#bert-embedder-reference-config">bert-embedder parameters</a>.
</p>

<h3 id="onnx-debug">Debugging ONNX models</h3>
<p>
  When loading <a href="https://onnx.ai/">ONNX</a> models for embedders, the model must have correct inputs and output parameters.
  Vespa offers tools to inspect ONNX model files.
  Here, <em>minilm-l6-v2.onnx</em> is in current working directory:
</p>
<pre>
$ docker run -v `pwd`:/w \
  --entrypoint /opt/vespa/bin/vespa-analyze-onnx-model \
  vespaengine/vespa \
  /w/minilm-l6-v2.onnx

...
model meta-data:
  input[0]: 'input_ids' long[batch][sequence]
  input[1]: 'attention_mask' long[batch][sequence]
  input[2]: 'token_type_ids' long[batch][sequence]
  output[0]: 'output_0' float[batch][sequence][384]
  output[1]: 'output_1' float[batch][384]
...
</pre>
<p>
The above model input and output names conforms with the
default <a href="reference/embedding-reference.html#bert-embedder-reference-config">bert-embedder parameters</a>.
</p>
<p>
  If loading models without the expected input and output parameter names, the Vespa Container node will not start
  (check <em>vespa.log</em> in the container running Vespa):
</p>
<pre>
 WARNING container        Container.com.yahoo.container.di.Container
  Caused by: java.lang.IllegalArgumentException: Model does not contain required input: 'input_ids'. Model contains: input
</pre>
<p>
  When this happens, a deploy looks like:
</p>
<pre>
$ vespa deploy --wait 300
Uploading application package ... done

Success: Deployed .

Waiting up to 5m0s for query service to become available ...
Error: service 'query' is unavailable: services have not converged
</pre>

<h2 id="embedder-performance">Embedder performance</h2>
<p>Embedding inference can be resource intensive for larger embedding models. Factors that impacts performance:</p>

<ul>
  <li>The embedding model parameters. Larger models are more expensive to evaluate than smaller models.</li>
  <li>The sequence input length. Transformer type models scales quadratic with input length. Since queries
    are typically shorter than documents, embedding queries is less resource intensive than documents.
  </li>
  <li>
    The number of inputs to the <code>embed</code> call. When encoding arrays, consider how many inputs a single document can have.
    For CPU inference, increasing <a href="reference/document-v1-api-reference.html#timeout">feed timeout</a> settings
    might be required when documents have many <code>embed</code>inputs.
  </li>
</ul>
<p>Using <a href="reference/embedding-reference.html#embedder-onnx-reference-config">GPU</a>, especially for longer sequence lengths (documents), can dramatically improve performance and reduce cost.
  See the blog post on <a href="https://blog.vespa.ai/gpu-accelerated-ml-inference-in-vespa-cloud/">GPU-accelerated ML inference in Vespa Cloud</a>.
  See also <a href="/en/operations-selfhosted/vespa-gpu-container.html">Vespa container GPU setup</a> guide for using GPUs with Vespa.
</p>

<h2 id="metrics">Metrics</h2>
<p>
    Vespa's built-in embedders such as Bert and HuggingFace emits metrics for computation time and token sequence length.
    These metrics are prefixed with <code>embedder.</code>
    and listed in the <a href="reference/container-metrics-reference.html">Container Metrics</a> reference documentation.
    Third-party embedder implementations may inject the <code>ai.vespa.embedding.EmbedderRuntime</code> component to easily emit the same predefined metrics,
    although emitting custom metrics is perfectly fine.
</p>
