---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Embedding"
redirect_from:
- /documentation/embedding.html
---

<p>
  A common technique in modern big data serving applications is to map unstructured data - say, text or images -
  to points in an abstract vector space and then do computation in that vector space. For example, retrieve
  similar data by <a href="approximate-nn-hnsw.html">finding nearby points in the vector space</a>,
  or <a href="onnx.html">using the vectors as input to a neural net</a>.
  This mapping is usually referred to as <em>embedding</em>.
  Read more about embedding and embedding management in this 
  <a href="https://blog.vespa.ai/tailoring-frozen-embeddings-with-vespa/">blog post</a>.
</p>

<p>Without using Vespa native embedding support, the embedding vectors must be provided by the client like below:</p>
<img src="/assets/img/vespa-overview-embeddings-1.svg" alt="document- and query-embeddings"
  width="1000" height="auto"/>

  <p>
  By using the embedding features in Vespa, developers can instead generate the embeddings
  into the Vespa document and query flows.
  Observe how <code>embed</code> is used to create the vector embeddings from text in the schema and queries:
</p>
<img src="/assets/img/vespa-overview-embeddings-2.svg" alt="Vespa's embedding feature, creating embeddings from text"
  width="800" height="auto"/>
<p>
  Integrating embedding <em>into</em> the Vespa application simplify the serving architecture,
  as well as cut serving latencies significantly - vector data transfer is minimized, with fewer components and serialization overhead.
</p>

<h2 id="provided-embedders">Provided embedders</h2>
<p>
  Vespa provides several embedders as part of the platform.
</p>
<p>An example of a Vespa <a href="query-api.html#http">query</a> request using Vespa embed functionality:</p>
<pre>{% highlight json %}
 {
    "yql": "select * from doc where {targetHits:10)nearestNeighbor(embedding,query_embedding)",
    "query": "semantic search",
    "input.query(query_embedding)": "embed(semantic search)",
}
{% endhighlight %}</pre>
<p>
  See more usage examples in <a href="#embedding-a-query-text">embedding a query text</a>
  and <a href="#embedding-a-document-field">embedding a document field</a>.
</p>

<h3 id="huggingface-embedder">Huggingface Embedder</h3>
<p>
  An embedder using any <a href="https://huggingface.co/docs/tokenizers/index">Huggingface tokenizer</a>,
  including multilingual tokenizers,
  to produce tokens which is then input to a supplied transformer model in ONNX model format.
</p>
<p>
  The Huggingface Embedder provides embeddings directly suitable for <a href="approximate-nn-hnsw.html">retrieval</a>
  and <a href="ranking.html">ranking</a> in Vespa, and makes it easy
  to implement semantic search with no need for custom components or client-side embedding inference
  when used with the syntax for invoking the embedder in queries and during document indexing described
  in <a href="#embedding-a-query-text">embedding a query text</a> and <a href="#embedding-a-document-field">embedding a document field</a>.
</p>
<p>
  The Huggingface embedder is configured in <a href="reference/services.html">services.xml</a>,
  within the <code>container</code> tag:
</p>
<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="hf-embedder" type="hugging-face-embedder">
        <transformer-model path="my-models/model.onnx"/>
        <tokenizer-model path="my-models/tokenizer.json"/>
    </component>
    ...
</container>
{% endhighlight %}</pre>
<ul>
  <li>
    The <code>transformer-model</code> specifies the embedding model in <a href="https://onnx.ai/">ONNX</a>.
    See <a href="https://github.com/vespa-engine/sample-apps/blob/master/simple-semantic-search/export_hf_model_from_hf.py">export_hf_model_from_hf.py</a>,
    for how to export embedding models from Huggingface to <a href="https://onnx.ai/">ONNX</a> format.
  </li>
  <li>
    The <code>tokenizer-model</code> specifies the Huggingface <code>tokenizer.json</code> formatted file.
    See <a href="https://huggingface.co/transformers/v4.8.0/fast_tokenizers.html#loading-from-a-json-file">HF loading tokenizer from a json file.</a>
  </li>
</ul>
<p>
  When using <code>path</code>, the model files must be supplied in the Vespa
  <a href="application-packages.html#deploying-remote-models">application package</a>.
  The above example uses files in the the <code>models</code> directory,
  or specified by <code>model-id</code> when deployed on <a href="https://cloud.vespa.ai/en/model-hub">Vespa Cloud</a>.
  See <a href="reference/embedding-reference.html#model-config-reference">model config reference</a>.
</p>
<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="e5" type="hugging-face-embedder">
        <transformer-model path="my-models/model.onnx"/>
        <tokenizer-model url="https://huggingface.co/intfloat/e5-base-v2/raw/main/tokenizer.json"/>
    </component>
    ...
</container>{% endhighlight %}</pre>
<p>See <a href="reference/embedding-reference.html#huggingface-embedder-reference-config">configuration reference</a> for all the parameters.</p>

<h4 id="huggingface-embedder-models">Huggingface embedder models</h4>
<p>
  The following are examples of text embedding models that can be used with the hugging-face-embedder
  and their output <a href="tensor-user-guide.html">tensor</a> dimensionality.
  The resulting <a href="reference/tensor.html#tensor-type-spec">tensor type</a> can be either <code>float</code>
  or <code>bfloat16</code>:
</p>
  <ul>
    <li><a href="https://huggingface.co/intfloat/e5-small-v2">intfloat/e5-small-v2</a> produces <code>tensor&lt;float&gt;(x[384])</code></li>
    <li><a href="https://huggingface.co/intfloat/e5-base-v2">intfloat/e5-base-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/intfloat/e5-large-v2">intfloat/e5-large-v2 produces <code>tensor&lt;float&gt;(x[1024])</code></a></li>
    <li><a href="https://huggingface.co/intfloat/multilingual-e5-base">intfloat/multilingual-e5-base</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">sentence-transformers/all-MiniLM-L6-v2</a> produces <code>tensor&lt;float&gt;(x[384])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2">sentence-transformers/all-mpnet-base-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2">sentence-transformers/paraphrase-multilingual-mpnet-base-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
  </ul>
<p>
  All of these example text embedding models can be used in combination with Vespa's
  <a href="nearest-neighbor-search.html">nearest neighbor search</a>
  using <a href="reference/schema-reference.html#distance-metric">distance-metric</a> <code>angular</code>.
</p>
<p>
  Check the <em>Massive Text Embedding Benchmark </em> (MTEB) benchmark and
  <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB leaderboard</a>
  for help with choosing an embedding model.
</p>


<h3 id="bert-embedder">Bert embedder</h3>
<p>
  An embedder using the <a href="#wordpiece-embedder">WordPiece</a> embedder to produce tokens
  which is then input to a supplied <a href="https://onnx.ai/">ONNX</a> model on the form expected by a BERT base model.
  The Bert embedder is limited to English (wordpiece) and BERT-styled transformer models with three model inputs
  (<em>input_ids, attention_mask, token_type_ids</em>).
  Prefer using the <a href="#huggingface-embedder">Huggingface Embedder</a> instead of the Bert embedder.
</p>
<p>
  The Bert embedder is configured in <a href="reference/services.html">services.xml</a>,
  within the <code>container</code> tag:
</p>
<pre>{% highlight xml %}
<container version="1.0">
  <component id="myBert" type="bert-embedder">
    <transformer-model path="models/e5-small-v2.onnx"/>
    <tokenizer-vocab url="https://huggingface.co/intfloat/e5-small-v2/raw/main/vocab.txt"/>
    <max-tokens>128</max-tokens>
  </component>
</container>
{% endhighlight %}</pre>
<ul>
  <li>
    The <code>transformer-model</code> specifies the embedding model in <a href="https://onnx.ai/">ONNX</a>.
    See <a href="https://github.com/vespa-engine/sample-apps/blob/master/simple-semantic-search/export_model_from_hf.py">export_model_from_hf.py</a>,
    for how to export embedding models from Huggingface to <a href="https://onnx.ai/">ONNX</a> format.
  </li>
  <li>
    The <code>tokenizer-vocab</code> specifies the Huggingface <code>vocab.txt</code> file, with one valid token per line.
    Note that the Bert embedder does not support the <code>tokenizer.json</code> formatted tokenizer configuration files.
    This means that tokenization settings like max tokens should be set explicitly. 
  </li>
</ul>
<p>See <a href="reference/embedding-reference.html#bert-embedder-reference-config">configuration reference</a> for all configuration options. </p>

<h2 id="embedding-a-query-text">Embedding a query text</h2>
<p>Where you would otherwise supply a tensor representing the vector point in a query,
you can with an embedder configured instead supply any text enclosed in <code>embed()</code>, e.g:</p>

<pre>
input.query(myEmbedding)=<span class="pre-hilite">embed(myEmbedderId, "Hello%20world")</span>
</pre>
<p>If you have only configured a single embedder, you can skip the embedder id argument and optionally also the quotes.
Both single and double quotes are permitted. Note that query <a href="reference/schema-reference.html#inputs">input tensors</a>
must be defined in the schema's rank-profile. See <a href="reference/schema-reference.html#inputs">schema reference inputs</a>.</p>
<pre>
inputs {
  query(myEmbedding) tensor&lt;float&gt;(x[768])
}
</pre>
<p>Output from <code>embed</code> that cannot fit into the tensor dimensionality is truncated, only retaining the first values.</p>

<p>A single Vespa <a href="query-api.html#http">query</a> can use multiple embedders or embed multiple texts with the same embedder:</p>
<pre>{% highlight json %}
 {
    "yql": "select * from sources * where ...",
    "query": "semantic search",
    "input.query(embedding)": "embed(e5, contextualized search)",
    "input.query(embedding2)": "embed(e5, neural search)"
    "ranking": "semantic",
}{% endhighlight %}</pre>

<h2 id="embedding-a-document-field">Embedding a document field</h2>
<p>Use the Vespa <a href="reference/advanced-indexing-language.html#statement">indexing language</a>
to convert one or more string fields into an embedding vector by using the <code>embed</code> function,
for example:</p>

<pre>
schema doc {

    document doc {

        field title type string {
            indexing: summary | index
        }

        field body type string {
            indexing: summary | index
        }

    }

    field embeddings type tensor&lt;bfloat16&gt;(x[384]) {
        indexing {
            (input title || "") . " " . (input body || "") | <span class="pre-hilite">embed embedderId</span> | attribute | index
        }
        index: hnsw
    }

}
</pre>
<p>
  The above example uses two input fields and concatenate them into a single input string to the embedder.
  See <a href="reference/advanced-indexing-language.html#choice-example">indexing choice</a> for details.
</p>

<p>If each document has multiple text segments, represent them in an array and store the vector embeddings
in a tensor field with one mapped and one indexed dimension.
The array indexes (0-based) are used as labels in the mapped tensor dimension.
See <a href="https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/">Revolutionizing Semantic Search with Multi-Vector HNSW Indexing in Vespa</a>.
</p>

<pre>
schema doc {

    document doc {

        field chunks type array&lt;string&gt; {
            indexing: index | summary
        }

    }

    field embeddings type tensor&lt;bfloat16&gt;(p{},x[5]) {
        indexing: input chunks | <span class="pre-hilite">embed embedderId</span> | attribute | index
        index: hnsw
    }

}
</pre>
<p>If you only have configured a single embedder you can skip the embedder id argument.</p>

<p>
  The indexing expression can also use <code>for_each</code> and include other document fields.
  For example the <em>E5</em> family of embedding models uses instructions along with the input. The following
  expression prefixes the input with <em>passage: </em> followed by a concatenation of the title and a text chunk.</p>

<pre>
schema doc {

    document doc {

        field title type string {
            indexing: summary | index
        }

        field chunks type array&lt;string&gt; {
            indexing: index | summary
        }

    }
    field embedding type tensor&lt;bfloat16&gt;(p{}, x[384]) {
        indexing {
            input chunks |
                for_each {
                    "passage: " . (input title || "") . " " . ( _ || "")
                } | embed e5 | attribute | index
        }
        attribute {
            distance-metric: prenormalized-angular
        }
    }
}
</pre>
<p>
  See <a href="reference/advanced-indexing-language.html#execution-value-example">Indexing language execution value</a>
  for details.
</p>

{% include important.html content='Note that the generated embedding fields
are defined outside the <code>document</code> clause in the schema.' %}


<h2 id="examples">Examples</h2>
<p>
  Try the <a href="https://github.com/vespa-engine/sample-apps/tree/master/simple-semantic-search">
  simple-semantic-search</a> sample application.
  The <a href="https://github.com/vespa-engine/sample-apps/tree/master/commerce-product-ranking">commerce-product-ranking</a>
  demonstrates using multiple embedders. The
  <a href="https://github.com/vespa-engine/sample-apps/tree/master/multi-vector-indexing">multi-vector-indexing</a> sample-app demonstrates
  how to use embedders with multiple inputs.
</p>

<h2 id="embedder-performance">Embedder performance</h2>
<p>Embedding inference can be resource intensive for larger embedding models. Factors that impacts performance:</p>

<ul>
  <li>The embedding model parameters. Larger models are more expensive to evaluate than smaller models.</li>
  <li>The sequence input length. Transformer type models scales quadratic with input length.
  </li>
  <li>
    The number of inputs to the <code>embed</code> call. When encoding arrays, consider how many inputs a single document can have.
    For CPU inference, increasing <a href="reference/document-v1-api-reference.html#timeout">feed timeout</a> settings
    might be required when documents have many <code>embed</code>inputs.
  </li>
</ul>
<p>Using GPU, especially for longer sequence lengths (documents), can dramatically improve performance and reduce cost.
  See the blog post on <a href="https://blog.vespa.ai/gpu-accelerated-ml-inference-in-vespa-cloud/">GPU-accelerated ML inference in Vespa Cloud</a>.
</p>
