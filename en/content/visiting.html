---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Visiting"
redirect_from:
- /documentation/content/visiting.html
---

<p>
  Visiting is a feature to efficiently process a set of documents, identified by a
  <a href="../reference/document-select-language.html">document selection expression</a>.
  See <a href="#request-handling">request handling</a> for details on how the feature works.
</p>
<p>
  In short, <em>visit</em> iterates over all, or a set of, <a href="buckets.html">buckets</a>
  and sends documents to (a set of) targets.
  The target is normally the visit client
  (like <a href="../reference/vespa-cmdline-tools.html#vespa-visit">vespa-visit</a>),
  but can be set a set of targets that act like sinks for the documents -
  see <a href="../reference/vespa-cmdline-tools.html#vespa-visit-target">vespa-visit-target</a>.
</p>
<p>
  Typically, the visit use cases are not time sensitive, like data reprocessing,
  and document dump for backup and cluster clone -
  <a href="https://cloud.vespa.ai/en/cloning-applications-and-data">cloning applications and data</a>
  is a good read for more details.
</p>
<p>
  Visiting does not have snapshot isolation—it returns the state of documents as they
  were when iterated over. Iteration order is implementation-specific.
  See <a href="consistency.html#read-consistency">the consistency documentation</a> for more details.
</p>
{% include note.html content='Due to the bucket iteration, visiting is normally a high-latency iteration.
  Even an empty content cluster is pre-partitioned into many data buckets to enable scaling.
  Dumping just one document requires iteration over all data buckets,
  unless <em>location</em>-specific selections are used.
  To test visiting performance, use larger data sets; do not extrapolate from small.
  Use the <a href="../query-api.html">query API</a> for low latency operations on small result sets.' %}



<h2 id="tools">Tools</h2>
<p>
<a href="../reference/vespa-cmdline-tools.html#vespa-visit">vespa-visit</a> is used to run a visit operation.
By default, vespa-visit gets visited documents and emits to stdout.
However, the tool may specify a visitor target and be used as a tool
to run reprocessing or migration.
It supports keeping a progress file on disk,
such that you can restart it if it should fail in the middle for some reason.
</p><p>
<a href="../reference/vespa-cmdline-tools.html#vespa-visit-target">vespa-visit-target</a>
is a tool to set up an endpoint for visiting data.
It binds to a socket or to a slobrok address,
which is specified as a target in the visit client.
</p><p>
For programmatic access, see the
<a href="../document-api-guide.html#visitorsession">visitor session API</a>.
</p>



<h2 id="selection">Selection</h2>
<p>
To select which documents to visit, use a
<a href="../reference/document-select-language.html">document selection expression</a>.
</p><p>
If co-localization of documents has been used, and the selection specifies location criteria,
the visitor client may be able to identify one or a few buckets
that are guaranteed to include all documents that will match the expression.
In all other cases, visiting needs to visit all buckets in the cluster.
</p><p>
Note that streaming search typically relies on this behavior to ensure low latency,
but the visitor client will only be able to map visitors to a bucket subset
if the document selection is simple, i.e. maps to a <em>location</em>.
If streaming search uses a lot of time,
verify that the selection is actually simple enough
for the visitor client to map it to a small bucket set.
</p>



<h2 id="fields">Fields</h2>
<p>
To select which fields to output, use a
<a href="../documents.html#fieldsets">fieldset</a>.
See examples below - common use cases is to use a comma-separated list of fields
or the <em>[document]</em> / <em>[all]</em> shorthand.
</p>

<h2 id="timestamp-ranges">Timestamp ranges</h2>
<p>
The <code>vespa-visit</code> tool has options for only returning documents that were
last modified within a particular timestamp range. Either—or both—of <code>--from <em>X</em></code>
and <code>--to <em>Y</em></code> may be specified, where <em>X</em> and <em>Y</em> are
microseconds from epoch in UTC wall-clock time.
</p>
<p>
Setting a timestamp range is only a <em>filter</em> on the document set that would otherwise
be returned by a visitor without a timestamp range. It does <me>not</em> imply snapshot isolation.
The returned set of documents may be affected by concurrent modifications to documents, as any
modification updates the document timestamp.
</p>

<h2 id="visiting-tombstones">Visiting tombstones</h2>
<p>
The <code>vespa-visit</code> tool can optionally also return tombstone entries of documents
that have been removed by specifying <code>--visitremoves</code>. Tombstones will be output
as <code>remove</code> objects which only contain a document ID. Vespa does not retain,
nor return, the document data of removed documents. When using <code>--visitremoves</code>
regular (non-tombstone) documents will also be returned.
</p>
<p>
Note that tombstones are subject to automatic garbage collection—see
<a href="../operations/admin-procedures.html#data-retention-vs-size">data retention vs. size</a>.
</p>

<h2 id="data-export-import">Data export / import</h2>
<p>
Export data to stdout.
As a full data dump takes time / space / resources, refine as needed:
<!-- ToDo: Add example with vespa-cli once slicing is implemented there -->
</p>
<p>
On Vespa 8.133 and beyond:
</p>
<pre>
$ vespa-visit -v -p progress-file --slices 100 --sliceid 0 > 0.json
</pre>
<ul>
  <li><code>-v</code> prints % finished to stderr.</li>
  <li><code>-p progress-file</code> saves progress, allowing the visitor to resume at next startup.
    Always remove the progress file to run the visiting operation from the start.</li>
  <li><code>--slices 100 --sliceid 0</code> dumps 1% of the corpus by efficiently iterating
      over only 1/100th of the data space. For a given number of <code>--slices</code>, it's
      possible to visit the entire corpus (possibly in parallel) with non-overlapping output
      by visiting with all <code>--sliceid</code> values from (and including) 0 up to (and excluding)
      <code>--slices</code>.</li>
</ul>
<p>
On Vespa versions prior to 8.133:
</p>
<pre>
$ vespa-visit -v -p progress-file -s 'id.hash().abs() % 100 = 0' > 0.json
</pre>
<ul>
  <li><code>-v</code> prints % finished to stderr.</li>
  <li><code>-p progress-file</code> saves progress, allowing the visitor to resume at next startup.
    Always remove the progress file to run the visiting operation from the start.</li>
  <li><code>-s 'id.hash().abs() % 100 == 0'</code> dumps 1% of the corpus - see <a href="#selection">selection</a>.
      Note that this expression is evaluated for <em>every</em> document in the cluster, so running 100
      visits comparing against all values in [0, 99) ends up reading all documents 100 times. Prefer
      using <code>--slices</code> and <code>--sliceid</code> instead if available.</li>
</ul>
{% include note.html content='If you have <a href="../reference/services-content.html#document">global
  document types</a>, you must add <code>--bucketspace global</code> to visit these documents.
  By default, visiting only iterates over non-global documents.' %}
<p>Import into a cluster using the <a href="../vespa-feed-client.html">vespa-feed-client</a>:</p>
<pre>
$ vespa-feed-client --file 0.json --endpoint https://container-endpoint:443/
</pre>
<p>
  Or see next section, routing output directly to another Vespa cluster.
</p>



<h2 id="data-migration-synchronization">Data migration / synchronization</h2>
<p>
To migrate a set of documents from one cluster to another, use <em>visiting</em> -
as the data is transferred directly, using a compact
serialization format, from the source nodes to the targets,
this is performance optimal (data is not piped through the visit client).
Implement backup this way, or dump to file.
</p><p>
  Search node recovery: Feed the documents directly to a search cluster.
  Example, selecting documents of type <em>music</em>:
</p>
<pre>
$ vespa-visit --selection music --datahandler indexing
</pre>
<p>
This feeds from the source into the search cluster in the same application.
Note that simultaneous feed can make updates go lost.
</p><p>
Include <a href="../operations/admin-procedures.html#data-retention-vs-size">remove-entries</a>
in the visit operation using <em>--visitremove</em> - this dumps the tombstones
of documents recently removed.
</p><p>
The <a href="../reference/routingpolicies.html#content">content policy</a> can be configured to
use a set of configuration servers from another cluster to configure
itself. This is specified with the <em>config</em> parameter. As
an example, the following route routes to the content cluster
<em>mycluster</em> with a configuration server on
<em>myconfigserver.mydomain.com:12345</em>:
<pre>
[Content:config=tcp/myconfigserver.mydomain.com:12345;cluster=mycluster]
</pre>
<p>
  The following examples illustrate how to copy all data from a source cluster to another cluster using vespa-visit:
</p>
<pre>
# Copies all data in the local cluster, routing it to the remote <em>mycluster</em>
$ vespa-visit --datahandler '[Content:config=tcp/myconfigserver.mydomain.com:12345;cluster=mycluster]'

# Limit to 'music' documents only
$ vespa-visit --datahandler '[Content:config=tcp/myconfigserver.mydomain.com:19070;cluster=mycluster]' \
  --selection music

# Limit to all documents for user '1234'
$ vespa-visit --datahandler '[Content:config=tcp/myconfigserver.mydomain.com:12345;cluster=mycluster]' \
  --selection id.user=1234
</pre>



<h2 id="reprocessing">Reprocessing</h2>
<p>
Reprocessing is used to solve these use cases:
</p>
<ul>
  <li>
    To change the document type used in ways that will not be
    backward compatible, define a new type, and reprocess to change all
    the existing documents.
  </li><li>
    Document identifiers can be changed to a new scheme.
  </li><li>
    Search documents can be reindexed after indexing steps have been changed.
  </li>
</ul>
<p>
This example illustrates how one can identify a subset of the
documents in a content cluster, reprocess these, and write them back.
It is assumed that a Vespa cluster is set up, with data.
</p>


<h3 id="set-up-a-document-reprocessing-cluster">1. Set up a document reprocessing cluster</h3>
<p>This example document processor:</p>
<ul>
  <li>deletes documents with an <em>artist</em> field whose value contains <em>Metallica</em></li>
  <li>uppercases <em>title</em> field values of all other documents</li>
</ul>
<pre>{% highlight java %}
import com.yahoo.docproc.Arguments;
import com.yahoo.docproc.DocumentProcessor;
import com.yahoo.docproc.Processing;
import com.yahoo.docproc.documentstatus.DocumentStatus;
import com.yahoo.document.DocumentOperation;
import com.yahoo.document.DocumentPut;
import com.yahoo.document.Document;

/**
 * Example of using a document processor will modify and/or delete
 * documents in the context of a reprocessing use case.
 */
public class ReProcessor extends DocumentProcessor {
    private String deleteFieldName;
    private String deleteRegex;
    private String uppercaseFieldName;

    public ReProcessor() {
        deleteFieldName = "artist";
        deleteRegex = ".*Metallica.*";
        uppercaseFieldName = "title";
    }

    public Progress process(Processing processing) {
        Iterator<DocumentOperation> it = processing.getDocumentOperations().iterator();
        while (it.hasNext()) {
            DocumentOperation op = it.next();
            if (op instanceof DocumentPut) {
                Document doc = ((DocumentPut) op).getDocument();

                // Delete the current document if it matches:
                String deleteValue = (String) doc.getValue(deleteFieldName);
                if (deleteValue != null) {
                    if (deleteValue.matches(deleteRegex)) {
                        it.remove();
                        continue;
                    }
                }

                // Uppercase the other field:
                String uppercaseValue = doc.getValue(uppercaseFieldName).toString();
                if (uppercaseValue != null) {
                    doc.setValue(uppercaseFieldName, uppercaseValue.toUpperCase());
                }
            }
        }
        return Progress.DONE;
    }
}
{% endhighlight %}</pre>
<p>
To compile this processor, see the <a href="../developer-guide.html">Developer Guide</a>.
For more information on document processing, refer
to <a href="../document-processing.html">Document processor Development</a>.
After having changed the Vespa setup, reload config:
</p>
<pre>
$ vespa-deploy prepare music
$ vespa-deploy activate
</pre>
<p>Restart nodes as well to activate.</p>


<h3 id="select-documents">2. Select documents</h3>
<p>
Define a selection criteria for the documents to be reprocessed.
(To reprocess <em>all</em> documents, skip this).
For this example, assume all documents where the field <em>year</em> is greater than 1995.
The selection string <em>music.year &gt; 1995</em> does this.
</p>


<h3 id="set-route">3. Set route</h3>
<p>
The visitor sends documents to a <a href="../routing.html">Messagebus route</a> - examples:
</p>
<ul>
<li><strong>default</strong> - Documents are sent to the <em>default</em>
    route.</li>
<li><strong>indexing</strong> - Documents are sent to <em>indexing</em>.</li>
<li><strong>&lt;clustername&gt;/chain.&lt;chainname&gt;</strong>:
    Documents are sent to the document processing chain <em>chainname</em> running in
    cluster <em>clustername</em>.</li>
</ul>
<p>
Assume you have a container cluster with id <em>reprocessing</em>
containing a docproc chain with id <em>reprocessing-chain</em>.
This example route sends documents from the content node,
into the document reprocessing chain, and ultimately, into indexing:
</p>
<pre>
reprocessing/chain.reprocessing-chain indexing
</pre>
<p>Details: <a href="../routing.html">Message Bus Routing Guide</a>.</p>


<h3 id="reprocess">4. Reprocess</h3>
<p>Start reprocessing:</p>
<pre>
$ vespa-visit -v --selection "music.year &gt; 1995" \
  --datahandler "reprocessing/chain.reprocessing-chain indexing"
</pre>
<p>The '-v' option emits progress information on standard error.</p>


<h3 id="notes">Notes</h3>
<p>
With multiple search clusters with one document type each,
and one content cluster storing all document types,
there are a few adjustments that must be made.
A visitor on a content node reads documents from disk,
and sends batches of documents in messages over the network.
If a visitor is set to visit all document types,
one message may thus contain more than one document type.
Since indexing is done in a separate indexing cluster for each search cluster,
these indexing clusters can only handle documents of their own type.
In the case where all documents regardless of document type are visited,
some documents will thus end up in the wrong indexing cluster, and indexing will fail.
In this use case, the solution is simple -
use a selection string to reprocess one document type at a time.
</p>



<h2 id="request-handling">Request handling</h2>
<h3 id="client">Client</h3>
<p>
If the selection criteria managed to map the visitor to a specific set of buckets,
these will be used when sending distributor visit requests.
If not, the visit client will iterate the entire bucket space,
typically at the minimum split-level required to decide correct distributor.
</p><p>
The distributors will receive the requests and look at what buckets it has
that are contained by the requested bucket.
If more than one, the distributor will only start a limited number of bucket visitors at the same time.
Once it has processed the first ones,
it will reply to the visitor client with the last bucket processed.
</p><p>
As all buckets have a natural order, the client can use the returned bucket as a progress counter.
Thus, after a distributor request has returned, the client knows one of the following:
</p>
<ul>
  <li>All buckets contained within the bucket sent have been visited</li>
  <li>All buckets contained within the bucket sent,
    up to this specific bucket in the order, have been visited</li>
  <li>No buckets existed that was contained within the requested bucket</li>
</ul>
<p>
The client can decide whether to visit in strict order,
allowing only one bucket to be pending at a time,
or whether to start visiting many buckets at a time, allowing great throughput.
</p>


<h3 id="distributor">Distributor</h3>
<p>
The distributors receive visit requests from clients for a given bucket,
which may map to none, one or many buckets within the distributor.
It picks one or more of the first buckets in the order,
picks out one content node copy of each
and passes the request on to the content nodes.
</p><p>
Once all the content node requests have been responded to,
the distributor will reply to the client with the last bucket visited,
to be used as a progress counter.
</p><p>
Subsequent client requests to the distributor will have the progress counter set,
letting the distributor ignore all the buckets prior to that point in the ordering.
</p><p>
Bucket splitting and joining does not alter the ordering,
and does thus not affect visiting much as long as the buckets are consistently split.
If two buckets are joined, where the first one have already been visited,
a visit request has to be sent to the joined bucket.
The content layer use the progress counter to avoid revisiting documents already processed in the bucket.
</p><p>
If the distributor only starts one bucket visitor at a time,
it can ensure the visitor order is kept.
Starting multiple buckets at a time may improve throughput and decrease latency,
but progress tracking will be less fine-grained,
so a bit more documents may need to be revisited when continued after a failure.
</p>


<h3 id="content-node">Content node</h3>
<p>
The content node receives visit requests for single buckets for which they store documents.
It may receive many in parallel, but their execution is independent of each other.
</p><p>
The visitor layer in the content node picks up the visit requests.
There it is assigned a visitor thread,
and an instance of the processor type is created for that visitor.
The processor will set up an iterator in the backend
and send one or more requests for documents to the backend.
</p><p>
The document selection specified in the visitor client is sent through to the backend,
allowing it to filter out unwanted data at the lowest level possible.
</p><p>
Once documents are retrieved from the backend, back up to the visitor layer,
the visit processor will process the data.
</p><p>
The default is one iterator request is pending to the backend at any time.
By sending many small iterator requests, having several pending at a time,
the processing may occur in parallel with the document fetching.
</p>


<h2 id="visitor-processor-types">Visitor processor types</h2>
<table class="table">
  <thead>
    <tr><th>Processor Type</th><th>Description</th></tr>
  </thead><tbody>
    <tr>
      <th>Dump visitor</th>
      <td>
        The most commonly used visitor processor type is the dump visitor.
        All it does is to send the read documents on to some external target
        specified by the visitor.
        Using the command line tool <em>vespa-visit</em>
        the default is to just send the documents back to the client,
        and have them printed to stdout.
        The dump visitor is used to implement reprocessing.
        Typically, using a messagebus route,
        that will send the documents through the document processing cluster
        and then back to the content cluster.
        Migration of documents from one cluster to another
        is also implemented using a dump visitor.
      </td>
    </tr><tr>
      <th style="white-space: nowrap">Streaming search visitor</th>
      <td>
        The <a href="../streaming-search.html">streaming search</a> visitor runs in the Vespa container,
        making it transparent whether search results were created from streaming or indexed search -
        see <a href="../reference/services-content.html#document">indexing mode</a>.
      </td>
    </tr>
  </tbody>
</table>



<h2 id="visitor-targets">Visitor targets</h2>
<p>
  Requests sent from the visitor processor are sent to a visitor target - types:
</p>
<table class="table">
  <thead>
    <tr><th>Target Type</th><th>Description</th></tr>
  </thead><tbody>
    <tr>
      <th style="white-space: nowrap">Message bus routes</th>
      <td>
        You can specify a <a href="../routing.html">message bus route</a> name directly,
        and this route will be used to send the results.
        This is typically used when doing reprocessing or migration.
        Message bus routes is set up in the application package.
        In addition, some routes may have been auto-generated in simple setups,
        for instance a route called <em>default</em> is generated
        if your setup is simple enough for the config model
        to likely guess where you want to send your data.
      </td>
    </tr><tr>
      <th>Slobrok address</th>
      <td><p>
        You can also specify a slobrok address for data to be sent to.
        A slobrok address is a slash-separated path
        where you can use asterisk to mean any element within this path.
        For instance, if you have a docproc cluster called <em>mydpcluster</em>,
        it will have registered its nodes with slobrok names like
        <em>docproc/cluster.mydpcluster/docproc/0/feed_processor</em>,
        where the 0 here indicates the first node in the cluster.
        You can thus specify to send visit data to this docproc cluster
        by stating a slobrok address of <em>docproc/cluster.mydpcluster/docproc/*/feed_processor</em>.
        Note that this will not send all the data to one or all the nodes.
        The data sent from the visitor will be distributed among the matching nodes,
        but each message will just be sent to one node.
        </p><p>
        Slobrok names can be used when using
        <a href="../reference/vespa-cmdline-tools.html#vespa-visit-target">vespa-visit-target</a>
        to retrieve the data at some location.
        If you start vespa-visit-target on two nodes,
        listening to slobrok names <em>mynode/0/visit-destination</em>
        and <em>mynode/1/visit-destination</em>,
        you can send the results to these nodes by specifying
        <em>mynode/*/visit-destination</em> as the data handler.
        </p><p>
        <a href="../reference/vespa-cmdline-tools.html#vespa-destination">vespa-destination</a>
        is similar to vespa-visit-target in that it can receive messages
        from messagebus and print the contents to stdout.
        It can be useful in situations where you want to debug a route or a docproc,
        by using the vespadestination as the endpoint of your route.</p>
      </td>
    </tr><tr>
      <th>TCP socket</th>
      <td>
        TCP sockets can also be specified directly.
        This requires that the endpoint speaks FNET RPC.
        This is typically done, either by using the <em>vespa-visit-target</em> tool,
        or by using a visitor destination programmatically by using utility class in the document API.
        A socket address looks like the following:
        tcp/<em>hostname</em>:<em>port</em>/<em>servicename</em>.
        For instance, an address generated by <em>vespa-visit-target</em>
        might look like: <em>tcp/myhost.mydomain.com:12345/visit-destination</em>
      </td>
    </tr>
  </tbody>
</table>



<h2 id="troubleshooting">Troubleshooting</h2>
<p>
  Normally all documents share the same <a href="buckets.html#bucket-space">bucket space</a>—documents
  for multiple schemas are co-located.
  When using <a href="../parent-child.html">parent/child</a>, global documents are stored in a separate bucket space.
  Use the <a href="../reference/document-v1-api-reference.html#bucketspace">bucketSpace</a> parameter
  to visit the <code>default</code> or <code>global</code> space.
  This is a common problem when dumping all documents and dumped count is not the expected count.
</p>
<p>
  A visit operation might stall/hang if the content cluster is in an inconsistent
  state—replicas are still merging between nodes.
  Find more details at
  <a href="../reference/vespa-cmdline-tools.html#visitinconsistentbuckets">visitinconsistentbuckets</a>.
</p>
