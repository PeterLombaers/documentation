---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Tensor Guide"
redirect_from:
- /documentation/tensor-user-guide.html
---

<p>
Vespa provides a <i>tensor</i> data model and computation engine to support advanced computations over data,
such as neural nets.
This guide explains the tensor support in Vespa. See also the <a href="reference/tensor.html">tensor reference</a>,
and our <a href="https://dl.acm.org/doi/10.1145/3459104.3459152">published paper</a>
(<a href="a_tensor_formalism_for_computer_science.pdf">pdf</a>).
</p>

<p>Content:</p>

<ul>
    <li><a href="#tensor-concepts">Tensor concepts</a>
    <li><a href="#tensor-document-fields">Tensor document fields</a>
    <li><a href="#feeding-tensors">Feeding tensors</a>
    <li><a href="#querying-with-tensors">Querying with tensors</a>
    <li><a href="#ranking-with-tensors">Ranking with tensors</a>
    <li><a href="#creating-tensors-from-document-fields">Creating tensors from document fields</a>
    <li><a href="#constant-tensors">Constant tensors</a>
    <li><a href="#performance-considerations">Performance considerations</a>
</ul>

<p>See also:</p>
<ul>
    <li><a href="onnx">Using ONNX models</a> to use machine-learned models taking tensor input in Vespa.
    <li>Some <a href="tensor-examples.html">examples of practical tensor computations</a>.
    <li><a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QGNIAaFDSPBdDTAF30gAkBTAJ2bAE8sBXMAgIYA7MDgA2AzmAGteQgCbSFYZgA8cbAJYBbZkNpgA7ptoALMLT0BnLKytLFloTdYr17K1c1ZnAOjAs7GCa9gJgVqa2BgA6QmoC2uLMxFy8-MJg0JrK2rYc2dC22gK03iIlYLGIZhxOLmA8VmxgAOY8mvLMALoAFKa0tDhWcAD0I-JYBFa+AG7MVjgCvgKaI3ojdbYAtI1sW20dzL792mIAlJBkqAC+V9ekGNTkuAxEDzQU+E8f9AiQAIIWay2YKhMDyEphKy0Vg8Ai0HhBMwVELSVp6NgCMSaABeJTKYCw0HCggkdhSc3htlCORKrE0BHm-liABVgXYwKYBHM0UIeNoAEbNIngnTWMr2ZEGQQiIUqEymZqdZwcEELGRNQmuMIELCC7L4ny+WL-ETxRJiVXEqGLOy1dlwS40MC3DD3K7fCDYShQZgkK5en2ezBCBjqu0AfU2rH9zq9fr+0Z6qmA1zOcGAAjgAEZiAK4AAma5Omium7vCDB72vWMfIMBugMQHKzXRowKiyGLCi3TOCUpTS+I5o4owzSqR0NssQd2PBvVv5+iuYetxqChv4t5hR9m152QBNQJOqRAFrrETiIADMXXTiEQuYLxBvxEQABZiABWYgANi6XRLN07mXKsXj+N551XONIF+KA2Wcal0hELkeQFLAzHCW1NWERQtx7cUfCsFIPB4MRSiEFpgnKMBtHHZhHHZfxAg4YwMJqMAAANoDELASg42IIVoMJaE4DRJ2dacXRA+cwN9PcVy+BsvQ3KBaNUeidwQmNl3IQ8YPZAAebjeNoAA+ZNUwva9bwzLMH2IJ8X3zd8v1-LpiynYCPRkn1IAg-coP3WDIHglx7BQjgoWYAwRVwTFSkI6QxB44wKMyEEwnkTQ5jtcJosJYldUSHghISoR-AAMRBbJoWERkUjCDiACssGyDjxNLLy533WSD3kwNFLXSAVMgfrMD0lrsh6cMmk0lwUjUjToxSaAegEPMzjWgAqAUzguTy3WknrfP8utBug4KWVMVFUWYABHdoZixPQDFoLAOqAw7vOOhgl0g879xGsb4zDLDtzbLaaLo+Q5tsQCbi61BQJOsbPioJSjwYKrXFyIJzSSIi8o4RAJimWZ5kWZZVnWaMtnxy1phOMRen6QZhjGUnpjmdUqbWIQNnZOnVASAnjloU59oku4UC6EBriAA">tensor playground</a>
which allows you to create and compute with tensors in your browser.
    <li>The <a href="https://javadoc.io/doc/com.yahoo.vespa/vespajlib/latest/com/yahoo/tensor/Tensor.html">Tensor Java API</a>.
</ul>

<h2 id="tensor-concepts">Tensor concepts</h2>

<p>
A tensor in Vespa is a data structure which generalizes scalars, vectors and matrices to any number of dimensions:
</p>
<ul>
  <li>A scalar is a tensor of rank 0</li>
  <li>A vector is a tensor of rank 1</li>
  <li>A matrix is a tensor of rank 2</li>
  <li>...</li>
</ul>

<p>
Tensors consist of a set of scalar valued <i>cells</i>, with each cell having a unique <i>address</i>.
A cell's address is specified by its index or label in all the dimensions of that tensor.
The number of dimensions in a tensor is the <em>rank</em> of the tensor.
Each dimension can be either <em>mapped</em> or <em>indexed</em>.
Mapped dimensions are sparse and allow any label (string identifier) designating their address,
while indexed dimensions use dense numeric indices starting at 0.
</p>

<p>
Example: Using <a href="reference/tensor.html#tensor-literal-form">literal form</a>, the tensor:
</p>
<pre>
{
    {x:2, y:1}:1.0,
    {x:0, y:2}:1.0
}
</pre>
<p>
has two dimensions named <code>x</code> and <code>y</code>, and has two cells with defined values:
</p>
<img src="/assets/img/tensor-guide.png" alt="Tensor graphical representation" />
<p>
A tensor has a <i>type</i>, which consists of a set of dimension names and types, and a value type.
The dimension name can be anything.

This defines a 2-dimensional mapped tensor (matrix) of floats:
<pre>
tensor&lt;float&gt;(topic{},segment{})
</pre>
This is a 2-dimensional indexed tensor (a 2x3 matrix) of double (double is the default value type):
<pre>
tensor(x[2],y[3])
</pre>
A combination of mapped and indexed dimensions is a <i>mixed</i> tensor:
<pre>
tensor&lt;float&gt;(key{},x[1000])
</pre>

<p>Vespa uses the type information to optimize execution plans at configuration time.
For dense data the best performance is achieved with indexed dimensions.</p>



<h2 id="tensor-document-fields">Tensor document fields</h2>

<p>Document fields in schemas can be of any tensor type:
<pre>
field tensor_attribute type tensor&lt;float&gt;(x[4]) {
    indexing: attribute | summary
}
</pre>



<h2 id="feeding-tensors">Feeding tensors</h2>

<p>Tensor field values can be added to documents in <a href="reference/document-json-format.html#tensor">
JSON format</a>. You can
<a href="reference/document-json-format.html#tensor-add">add</a>,
<a href="reference/document-json-format.html#tensor-remove">remove</a> and
<a href="reference/document-json-format.html#tensor-modify">modify</a> tensor cells, or
<a href="reference/document-json-format.html#tensor-field">assign</a> a completely new tensor value.
</p>

<p>From inside containers (e.g in a Document Process), you can also create Tensor values using the
<a href="https://javadoc.io/doc/com.yahoo.vespa/vespajlib/latest/com/yahoo/tensor/Tensor.html">tensor Java API</a>
to set tensor values in documents.
</p>



<h2 id="querying-with-tensors">Querying with tensors</h2>

<p>Tensors with one dense dimension can be indexed (HNSW) and used for searching (ANN), see
<a href="nearest-neighbor-search.html">nearest neighbor search</a>.</p>



<h2 id="ranking-with-tensors">Ranking with tensors</h2>

<p>Tensors can be used in making inferences and ranking over documents for a query, that is
in <a href="reference/ranking-expressions.html">ranking expressions</a> in
<a href="reference/schema-reference.html#rank-profile">rank profiles</a>.

<p>Accessing a document tensor in a ranking expressions is just like accessing any other attribute:
<code>attribute(tensor_attribute)</code>.</p>

<p>To pass tensors in queries, you need to
<a href="reference/schema-reference#inputs">define their type in the rank profile</a>.
Then you can either
<ul>
  <li>add it to the query in a <a href="searcher-development.html">Searcher</a> using the
      <a href="https://javadoc.io/doc/com.yahoo.vespa/vespajlib/latest/com/yahoo/tensor/Tensor.html">Tensor class</a>
      and setting it by <code>Query.getRanking().getFeatures.put("query(myTensor)", myTensorInstance)</code>, or
  <li>pass it in the request, using a parameter like <code>input.query(myTensor)</code> and
      passing a tensor value on the <a href="reference/tensor.html#tensor-literal-form">tensor literal form</a>.
</ul>

<p>You can then access the query tensor in ranking expressions by
<code>query(myTensor)</code>.</p>

<p>With these tensors, you can write functions in rank profiles which computes over them. For example:</p>
<pre>
rank-profile dot-product {

    first-phase {
        expression: dotProduct(query(tensor), attribute(tensor_attribute))
    }

    function dotProduct(a, b) {
        expression: sum(a*b)
    }

}
</pre>

<p>The tensor expression above is a short form for <code>reduce(join(a, b, f(x,y)(x * y) ), sum)</code>.
The full list of tensor functions are listed in the
<a href="reference/ranking-expressions.html#tensor-functions">ranking expression reference</a>.</p>


<h2 id="creating-tensors-from-document-fields">Creating tensors from document fields</h2>

<p>If you need to make tensor comutations using single-valued attributes, arrays or weighted sets,
you can convert them in a ranking expression:</p>

<ul>

  <li>Creating an <i>indexed</i> tensor where the <i>values</i> are lifted from single-value
  attributes can be done using the tensor <code>concat</code> function:
<pre>
concat(attribute(foo), attribute(bar))
</pre>
  This can also be used to concat single-value attributes to a tensor.
  </li>

  <li>Creating a <i>mapped</i> tensor where the <i>values</i> are lifted from single-value
  attributes can be done using the tensor generate function:
<pre>
tensor(x{}):{x1:attribute(foo), x2:attribute(bar)}
</pre>
  </li>

  <li>Creating a <i>mapped</i> tensor where the <i>label(s)</i> are lifted from a string array or
      single-value attribute can be done with the
      <a href="reference/rank-features.html#document-features">document feature.</a> <code>tensorFromLabels</code>.
  </li>

  <li>Creating a mapped tensor where the labels <i>and</i> values are lifted from a string array can be done with the
      <a href="reference/rank-features.html#document-features">document feature.</a> <code>tensorFromWeightedSet</code>.
  </li>
</ul>



<h2 id="constant-tensors">Constant tensors</h2>

<p>In addition to document tensors and query tensors,
<a href="reference/schema-reference.html#constant">constant tensors</a>
can be put in the <a href="reference/application-packages-reference.html">application package</a>.
This is useful when constant tensors are used in ranking expressions,
for instance machine learned models. Example:
</p>
<pre>
constants  {
    my_tensor_constant tensor&lt;float&gt;(x[10000]) file: constants/constant_tensor_file.json
}
</pre>
<p>
This defines a new tensor rank feature with the type as defined and the
contents distributed with the application package in the file
<em>constants/constant_tensor_file.json</em>. The format of this file is the
<a href="reference/document-json-format.html#tensor">tensor JSON format</a>, it can be compressed,
see the <a href="reference/schema-reference.html#constants">reference</a> for examples.
</p>

<p>
To use this tensor in a rank expression, encapsulate the constant name with <code>constant(...)</code>:
</p>
<pre>
rank-profile use_constant_tensor {
    first-phase {
        expression: sum(query(tensor) * attribute(tensor_attribute) * constant(tensor_constant))
    }
}
</pre>



<h2 id="tensors-with-strings">Tensors with strings</h2>

<p>Tensors in Vespa cannot have strings as values, since the mathematical tensor functions
would be undefined for such "tensors". However, you can still represent sets of strings in tensors
by using the strings as keys in a mapped tensor dimensions, using e.g 1.0 as values.
This allows you to perform set operations on strings and similar without making those tensors
incompatible with other tensors and with normal tensor operations.</p>



<h2 id="performance-considerations">Performance considerations</h2>

<p>Tensor expressions are fairly concise, and since the expressions themselves
are independent of the data size, the actual workload during ranking can be significant
for large tensors.</p>

<p>When using tensors in ranking it is important to have an understanding of the
potential computational cost for each query. As an example, assume
the dot product of two tensors with 1000 values each, e.g. <code>tensor&lt;double&gt;(x[1000])</code>.
Assuming one query tensor and one document tensor, the operation is:
</p>
<pre>
sum(query(tensor1) * attribute(tensor2))
</pre>
<p>
If 8 bytes is used to store each value (e.g. using a double), each tensor is approximately 8 KB.
With for instance <a href="https://ark.intel.com/content/www/us/en/ark/products/81055/intel-xeon-processor-e52683-v3-35m-cache-2-00-ghz.html">a Haswell architecture</a>
the theoretical upper memory bandwidth is 68 GB/s, which is around 9 million
document ranking evaluations per second.
With 1 million documents, this means the maximum throughput, in regard to pure memory bandwidth,
is 9 queries per second (per node).
</p><p>
Even though you would typically not do the above without reducing the
search space first (using matching and first phase), it is important to consider
the memory bandwidth and other hardware limitations when developing ranking expressions with tensors.
</p>
<h3 id="cell-value-types">Performance considerations for cell value types</h3>
<table class="table">
  <tr>
    <th>double</th>
    <td>
      <p id="double">
      The 64-bit floating-point "double"
      format is the default cell type.  It gives best precision at
      the cost of high memory usage and somewhat slower calculations.
      Using a smaller value type increases performance, trading off
      precision, so consider changing to one of the cell types below
      before scaling your application.
      </p>
    </td>
  </tr><tr>
    <th>float</th>
    <td>
      <p id="float">
      The usual 32-bit floating-point format "float" should usually
      be used for all tensors when scaling for production.
      (Note that other frameworks, like tensorflow, will also prefer 32-bit floats.)
      A vector with 1000 dimensions, <code>tensor&lt;float&gt;(x[1000])</code> would then
      use approx 4K memory per tensor value.
      </p>
    </td>
  </tr><tr>
    <th>bfloat16</th>
    <td>
      <p id="bfloat16">
      If memory (or memory bandwidth) is still a concern, it's
      possible to change the most space-consuming tensors to use
      the <code>bfloat16</code> cell type.  This type has the range
      as a normal 32-bit float but only 8 bits of precision, and
      can be thought of as "float with lossy compression".  See also
      <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16
      floating-point format</a> on Wikipedia.  Some careful analysis
      of your data is required before using this type.
      </p><p>
      Note that when doing calculations <code>bfloat16</code> will act as if
      it was a 32-bit float, but the smaller size comes with a potential
      computation overhead. In most cases, the <code>bfloat16</code> needs
      to be converted to a 32-bit float before the actual calculation can
      take place; adding an extra conversion step.
      </p><p>
      In some cases, having tensors with <code>bfloat16</code> cells might
      bypass some build-in optimizations in the back-end (like matrix
      multiplication) that will be hardware accelerated only if the cells
      are the same type.  To avoid this last case, you can use the
      <a href="reference/ranking-expressions.html#cell_cast">cell_cast</a>
      tensor operation to make sure the cells are of the
      appropriate type before doing the more expensive operations.
      </p>
    </td>
  </tr><tr>
    <th>int8</th>
    <td>
      <p id="int8">
      If one uses machine-learning to generate a model with data
      quantization you can target the <code>int8</code> cell value
      type, which is a signed integer with range from -128 to +127 only.
      This is also treated like a "float with limited range and lossy
      compression" by the Vespa tensor framework, and gives results as if it
      was a 32-bit float when any calculation is done.  This type is also
      suitable when representing boolean values (0 or 1).  Note that if the input
      for an <code>int8</code> cell is not directly representable, the
      resulting cell value is undefined, so you should take care to only
      input numbers in the <code>[-128,127]</code> range.
      It's also possible to use <code>int8</code> representing binary
      data for <a href="reference/schema-reference.html#distance-metric">hamming distance</a>
      Nearest-Neighbor search.
      </p>
    </td>
  </tr>
</table>
