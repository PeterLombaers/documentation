---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: Feed using Hadoop, Pig, Oozie
redirect_from:
- /documentation/feed-using-hadoop-pig-oozie.html
---

<p>
This document discusses methods to feed to Vespa from various grid resources.
</p><p>
Examples use a local file system.
Prerequisite: Have some data on a Hadoop cluster, ready to feed to Vespa.
This includes text or JSON files on HDFS, data in Hive or HBase table.
</p><p>
<code>vespa-hadoop</code> contains classes and utilities to enable feeding directly to Vespa endpoints.
It is a minimal layer over the <a href="vespa-feed-client.html">vespa-feed-client</a>. Dependency:
<pre>
&lt;dependency&gt;
  &lt;groupId&gt;com.yahoo.vespa&lt;/groupId&gt;
  &lt;artifactId&gt;vespa-hadoop&lt;/artifactId&gt;
  &lt;version&gt;7.543.30&lt;/version&gt; &lt;!-- Find latest version at <a href="https://search.maven.org/search?q=g:com.yahoo.vespa%20a:vespa-hadoop">search.maven.org/search?q=g:com.yahoo.vespa%20a:vespa-hadoop</a> --&gt;
&lt;/dependency&gt;
</pre>
  {% include important.html content="<code>vespa-hadoop</code> depends on the Apache HTTP client library.
  Hadoop also pulls in this library, possibly with a conflicting version.
  If jobs fail due to <em>NoSuchFieldErrors</em> or similar,
  try adding <code>-Dmapreduce.job.user.classpath.first=true</code>."%}



<h2 id="pig">Pig</h2>
<p>
<a href="https://pig.apache.org/">Apache Pig</a> is a platform for
analyzing large data sets that consists of a high-level language for expressing
data analysis programs, coupled with infrastructure for evaluating these programs.
Pig is the standard method for ETL and grid scale analysis at Yahoo.
Pig features methods to load data from different types of sources,
transform data, and store that data into various types of destinations.
</p><p>

The <em>vespa-hadoop</em> library provides the <code>VespaStorage</code> class
which provides a simple way of storing data into a Vespa endpoint. Basic usage:

<pre>
REGISTER path/to/vespa-hadoop.jar

DEFINE VespaStorage com.yahoo.vespa.hadoop.pig.VespaStorage(&lt;options&gt;);

A = LOAD '&lt;path&gt;' [USING &lt;storage&gt;] [AS &lt;schema&gt;];

-- apply any transformations

STORE A INTO '$ENDPOINT' USING VespaStorage();
</pre>
<p>
To run the above Pig script, specify <a href="#vespa.feed.endpoint">endpoint</a>.
The endpoint is the hostname to feed to.
Port 4080 currently the default setting, to override the port use <code>-Dvespa.feed.defaultport=8080</code>.
</p>
<p>
  <code>VespaStorage</code> supports feeding to multiple clusters.
  To specify multiple endpoints, separate them by commas.
  The VespaStorage will write the operations to all clusters and wait for a reply/ack from all,
  so it's a blocking implementation.
  If one cluster is down for some reason (e.g. maintenance),
  also the remaining healthy clusters stops getting operations.
</p>
<p>
As an example, assume the above script is called <code>feed.pig</code>.
Run it from a Hadoop node to two Vespa instances
(add <a href="#vespa.feed.proxy.host">vespa.feed.proxy.host/port</a> as needed):
</p>
<pre>
pig -x local -f feed.pig
    -p ENDPOINT=hostname-of-endpoint-1,hostname-of-endpoint-2
    -Dvespa.feed.proxy.host=proxy-host
    -Dvespa.feed.proxy.port=proxy-port
    -Dvespa.feed.defaultport=8080
</pre>


<h3 id="feeding-pre-constructed-operations">Feeding pre-constructed operations</h3>
<p>
In the case where the document operations are already formatted using the Vespa <a href="reference/document-json-format.html">JSON format</a>, one can
feed this directly to a Vespa endpoint:
<pre>
REGISTER vespa-hadoop.jar
DEFINE VespaStorage com.yahoo.vespa.hadoop.pig.VespaStorage();

-- Load data - one column for json data
data = LOAD 'feed-dump.data' AS (json:chararray);

-- Store into Vespa
STORE data INTO '$ENDPOINT' USING VespaStorage();
</pre>
The source, e.g 'feed-dump.data' needs to contain exactly one valid JSON document operation per line, e.g
<pre>
{"put": "id:namespace:music::0","fields": {"title":"foo"}}
{"put": "id:namespace:music::1","fields": {"title":"foo"}}
{"update": "id:namespace:music::2","fields": { "title": { "assign":"bar"}}}
</pre>
To store multi-line formatted JSON operations use the <em>VespaSimpleJsonLoader</em>:
<pre>
REGISTER vespa-hadoop.jar
DEFINE VespaJsonLoader com.yahoo.vespa.hadoop.pig.VespaSimpleJsonLoader();
DEFINE VespaStorage com.yahoo.vespa.hadoop.pig.VespaStorage();

DATA = LOAD 'operations_multiline_data.json' USING VespaJsonLoader() AS (data:chararray);

-- Store into Vespa
STORE DATA INTO '$ENDPOINT' USING VespaStorage();
</pre>


<h3 id="feeding-structured-data">Feeding structured data</h3>
<p>
Example <a href="schemas.html">schema</a>:
<pre>
schema music {
    document music {
        field album type string  { indexing: index }
        field artist type string { indexing: summary | index }
        field duration type int  { indexing: summary }
        field title type string  { indexing: summary | index }
        field year type int      { indexing: summary | attribute }
    }
}
</pre>
Sample data, tab separated:
<pre>
Bad     Michael Jackson Bad     1987    247
Recovery        Eminem  So Bad  2010    240
</pre>
When loading data in Pig, specify a schema for that data unless it can be inferred from the data source.
After selecting the fields to feed to Vespa,
a <a href="reference/document-json-format.html">document operation</a>
must be constructed. There are two ways this can be accomplished.
Let the <code>VespaDocumentOperation</code> UDF construct the operation:
<pre>
REGISTER vespa-hadoop.jar

-- Create valid Vespa put operations
DEFINE VespaPutOperation
       com.yahoo.vespa.hadoop.pig.VespaDocumentOperation(
            'operation=put',
            'docid=id:namespace:music::&lt;artist&gt;-&lt;year&gt;'
       );

DEFINE VespaStorage
       com.yahoo.vespa.hadoop.pig.VespaStorage();

-- Load data from any source - here we load using PigStorage
data = LOAD '&lt;hdfs-path&gt;' AS (album:chararray, artist:chararray, title:chararray, year:int, duration:int);

-- Transform tabular data to a Vespa document operation JSON format
data = FOREACH data GENERATE VespaPutOperation(*);

-- Store into Vespa
STORE data INTO '$ENDPOINT' USING VespaStorage();
</pre>
<p>
Here, the <code>VespaDocumentOperation</code> is defined and configured to create the document operation.
The notable part is the <code>docid=...</code> section,
which specifies a document id template for each document.
The fields inside the brackets will be replaced with the values in each record.
This is a simple search and replace operation,
define custom <a href="documents.html">document IDs</a> as needed.
<code>PUT</code> is  default, use (<code>REMOVE</code>, <code>UPDATE</code>) as well.
</p><p>
Additionally, all fields in the relation (here <code>data</code>) will be
converted into a JSON form with best-effort, and should be suitable for most cases.
It uses the Pig schema to determine type conversion, and the mapping between
Pig data types and JSON Vespa types are as follows:
</p>
<table class="table">
  <thead></thead><tbody>
    <tr><td><b>Pig type</b></td><td><b>JSON-Vespa type</b></td></tr>
    <tr><td>int</td><td>number</td></tr>
    <tr><td>long</td><td>number</td></tr>
    <tr><td>float</td><td>number</td></tr>
    <tr><td>double</td><td>number</td></tr>
    <tr><td>chararray</td><td>string</td></tr>
    <tr><td>bytearray</td><td>base64 encoded string</td></tr>
    <tr><td>boolean</td><td>boolean</td></tr>
    <tr><td>datetime</td><td>long - milliseconds since epoch</td></tr>
    <tr><td>biginteger</td><td>number</td></tr>
    <tr><td>bigdecimal</td><td>number</td></tr>
    <tr><td>tuple</td><td>array</td></tr>
    <tr><td>bag</td><td>array of arrays</td></tr>
    <tr><td>map</td><td>JSON object</td></tr>
  </tbody>
</table>
In this case, the first row in data above will be transformed to the following JSON:
<pre>
{
    "put": "id:namespace:music::Michael Jackson-1987",
    "fields": {
        "album": "Bad",
        "artist": "Michael Jackson",
        "duration": 247,
        "title": "Bad",
        "year": 1987
    }
}
</pre>
<p>
In case the <code>VespaDocumentOperation</code> does not fit the needs,
write a custom UDF (user defined function) in any of the supported
languages to transform the data into valid Vespa operations.
Refer to the <a href="https://pig.apache.org/docs/r0.14.0/udf.html">Pig documentation</a>
for more details on how to write UDFs.
</p><p>
As an alternative to the above approach, let the <code>VespaStorage</code>
class directly generate the document operations without using the above UDF:
<pre>
REGISTER vespa-hadoop.jar

-- Transform tabular data to a Vespa document operation JSON format
DEFINE VespaStorage
       com.yahoo.vespa.hadoop.pig.VespaStorage(
            'create-document-operation=true',
            'operation=put',
            'docid=id:namespace:music::&lt;artist&gt;-&lt;year&gt;'
       );

-- Load data from any source - here we load using PigStorage
data = LOAD '&lt;source&gt;' AS (&lt;schema&gt;);

-- transform and select fields

-- Store into Vespa
STORE data INTO '$ENDPOINT' USING VespaStorage();
</pre>
<p>
Here the required information to create document operations is added
to the configuration of the <code>VespaStorage</code> class.
Internally it uses the same code as the above approach, so results will be identical.
</p>


<h3 id="parameters">Parameters</h3>
<p>
Parameters for <code>VespaStorage</code> and <code>VespaDocumentOperation</code> are:
</p>
<table class="table">
  <thead>
    <tr><th>parameter</th><th>default</th><th>description</th></tr>
  </thead><tbody>
    <tr><th>create-document-operation</th>
      <td><em>false</em></td>
      <td><em>true</em> or <em>false</em>. Only valid for <code>VespaStorage</code></td>
    </tr>
    <tr><th>operation</th>
      <td><em>put</em></td>
      <td>Any <a href="reference/document-json-format.html">valid Vespa document operation</a></td>
    </tr>
    <tr><th>docid</th>
      <td></td>
      <td>A document id template. Replaces &lt;field-name&gt; with its value for every record</td>
    </tr>
    <tr><th>exclude-fields</th>
        <td></td>
        <td>A list of fields to exclude when creating document operations.
            The field is still available for the document id template.</td>
    </tr>
    <tr><th>condition</th>
      <td></td>
      <td>A <a href="reference/document-json-format.html#test-and-set">test-and-set condition</a> template. Replaces &lt;field-name&gt; with its value for every record</td>
    </tr>
    <tr><th>create-if-non-existent</th>
        <td><em>false</em></td>
        <td>If true, adds <code>create=true</code> for creating documents if they don't yet exist. Only valid with <code>operation=update</code>. </td>
    </tr>
    <tr><th>update-map-fields</th>
        <td></td>
        <td>A list of map-type fields to update individual entries in the map when creating document operations.</td>
    </tr>
    <tr><th>remove-map-fields</th>
        <td></td>
        <td>A list of map-type fields to remove individual entries in the map when creating document operations.</td>
    </tr>
    <tr><th>update-tensor-fields</th>
        <td></td>
        <td>A list of tensor-type fields to add or overwrite individual tensor cells when creating document operations.</td>
    </tr>
    <tr><th>remove-tensor-fields</th>
        <td></td>
        <td>A list of tensor-type fields to remove existing tensor fields in a document when creating document operations.</td>
    </tr>
    <tr><th>create-tensor-fields</th>
        <td></td>
        <td>A list of fields to format as tensors.</td>
    </tr>
    <tr><th>bag-as-map-fields</th>
        <td></td>
        <td>A list of bag with tuple fields to format as a map.</td>
    </tr>
    <tr><th>simple-array-fields</th>
        <td></td>
        <td>A list of fields containing a bag of single element tuples to format as an array.</td>
    </tr>
    <tr><th>simple-object-fields</th>
        <td></td>
        <td>A list of fields containing a tuple to format as a general JSON object which can be used for weightedsets, structs or maps.</td>
    </tr>
  </tbody>
</table>
<p>
The following is an example on using the above parameters to update map and tensor fields. Example schema:
<pre>
search album {
    document album {
        struct song {
            field name type string {}
            field duration type int {}
        }
        field songs type map&lt;int, song&gt; { indexing: summary }
        field song_popularity type tensor&lt;float&gt;(song{}) { indexing: attribute | summary }
    }
}
</pre>
Use update parameters on specific fields:
<pre>
REGISTER vespa-hadoop.jar

-- Transform tabular data to a Vespa document operation JSON format
DEFINE VespaStorage
       com.yahoo.vespa.hadoop.pig.VespaStorage(
            'create-document-operation=true',
            'operation=update',
            'docid=id:namespace:song::&lt;docId&gt;',
            'update-map-fields=songs',
            'update-tensor-fields=song_popularity'
       );

-- Load data from any source - here we load using PigStorage
data = LOAD '&lt;source&gt;' AS (&lt;schema&gt;);

-- output : {docId: chararray,songs: {t: (key: int,value: (name: chararray,duration: int))},song_popularity: map[]}
DESCRIBE data;

-- output: (1,{(1,(Together Forever,132))},[Song:1#10.5])
DUMP data;

</pre>
<p>
In this case, the data above will be transformed to the following JSON:
</p>
<pre>
{
    "update": "id:namespace:album::1",
    "create": true,
    "fields": {
        "songs{1}": {
            "assign": {
                "name": "Together Forever",
                "duration": 132
            }
        },
        "song_popularity": {
            "add": {
                "cells": [
                    {
                        "address": {
                            "song": 1
                        },
                        "value": 10.5
                    }
                ]
            }
        }
    }
}
</pre>


<p>
For the same data in an update operation, different parameters can be passed to VespaStorage to generate remove tensor and map field operations:
<pre>
REGISTER vespa-hadoop.jar

-- Transform tabular data to a Vespa document operation JSON format
DEFINE VespaStorage
       com.yahoo.vespa.hadoop.pig.VespaStorage(
            'operation=update',
            'docid=id:namespace:people::&lt;docId&gt;',
            'remove-map-fields=mapField',
            'remove-tensor-fields=tensorField',
            'exclude-fields-name,duration'
       );
</pre>
<p>
In this case, the following JSON is generated:
</p>
<pre>
{
    "update": "id:namespace:song::1",
    "fields": {
        "songs{1}": {
            "remove": 0
        },
        "song_popularity": {
            "remove": {
                "addresses": [
                    {"song": "1"}
                ]
            }
        }
    }
}
</pre>



<h3 id="feeding-status">Feeding status</h3>
<p>
<code>VespaStorage</code> does not check the schema  before data feeding.
If the schema does not match the document type, feeding fails.
</p><p>
<code>VespaStorage</code> emits a set of counters at the end of a Pig job,
and the values of these counters can differ than the ones reported by Pig.
The reason for this is that the number of records stored as
reported by Pig is the number of records sent to the feeder,
while the feeder feeds and gathers statistics asynchronously for those documents. Example:
<pre>
...
	Vespa Feed Counters
		Documents failed=0
		Documents ok=100000
		Documents sent=100000
		Documents skipped=1

...
Output(s):
Successfully stored 100001 records in: "&lt;endpoint&gt;"
...
</pre>
<p>
Here, Pig reports that it successfully stored 100001 records,
but <code>VespaStorage</code> skipped one operation, probably due to a formatting error and only 100000 operations was sent.
The skipped counter is incremented if <code>VespaStorage</code>
could not find a valid document id in the read record (e.g. due to JSON formatting issues).
Any errors can be found in the job logs.
</p>


<h3 id="efficiency">Efficiency</h3>
<p>
<code>VespaStorage</code> uses the
<a href="vespa-feed-client.html">vespa-feed-client</a>,
and as such feeds as efficiently as possible.
As <code>VespaStorage</code> runs in a Pig job which is compiled down to a MapReduce or Tez job,
a number of mappers or reducers will be used to cover the file if the input file is sufficiently large.
As each mapper will have their own client feeding to the endpoint, each with multiple threads,
make sure the cluster can take the load.
Each client will throttle to not overload the endpoint, but the clients does not know how many total clients there are so one can easily over-saturate 
feeding endpoints.
</p><p>
There are two primary factors controlling feed load: the number of mappers or
reducers, and the number of concurrent connections per mapper/reducer. For large jobs with lots of data one might consider 
controlling the number of splits and thus number of mappers to make based on the input file size. This can be done by 
controlling <a href="https://pig.apache.org/docs/latest/perf.html#combine-files">combine-files</a> by e.g adding
<pre>
pig -Dpig.maxCombinedSplitSize=53687091200 -f feed.pig
</pre>
<p>
This is for Yarn and Hadoop v2, and will combine the input so that each map processes about 50G of data. 
This setting effectively reduces the number of Vespa client instances,
this can also be inverted using a lower setting to get higher number of tasks.
See the Pig and Yarn documentation for more information.
</p><p>
To set the number of concurrent connections per mapper/reducer,
set <a href="#vespa.feed.connections">vespa.feed.connections</a>. The default is 1 and rarely needs to be tuned upward. 
</p>

Another way to limit the number of parallel tasks feeding to Vespa for large jobs is to use <em>PARALLEL</em> directive of PIG

<pre>
data = LOAD '/some/data/*' USING PigStorage('\u0001') AS (guid:chararray, text:chararray, score:double);
grouped_data = GROUP data by guid PARALLEL 100;
STORE grouped_data INTO '$ENDPOINT' USING VespaStorage();
</pre>

<h2 id="hadoop-mapreduce">Hadoop MapReduce</h2>
<p>
The <code>VespaStorage</code> Pig class builds upon an <code>OutputFormat</code> class that feeds to Vespa.
<code>OutputFormat</code>s are used in MapReduce jobs, for special needs one can use this class from a MapReduce job.
Example:
<pre>
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

public class VespaFeed {

    public static class FeedMapper extends Mapper&lt;LongWritable, Text, LongWritable, Text&gt;& {
        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            context.write(key, value);
        }
    }

    public static void main(String... args) throws Exception {

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "vespafeed");
        job.setJarByClass(VespaFeed.class);
        job.setMapperClass(FeedMapper.class);
        job.setMapOutputValueClass(Text.class);

        // Set output format class
        job.setOutputFormatClass(VespaOutputFormat.class);

        FileInputFormat.setInputPaths(job, new Path(args[0]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
</pre>
Run on Hadoop:
<pre>
hadoop jar my-jar.jar VespaFeed &lt;hdfs-path&gt;
    -Dvespa.feed.endpoint=endpoint
    -Dvespa.feed.defaultport=8080
    -Dvespa.feed.proxy.host=proxyhost
    -Dvespa.feed.proxy.port=proxyport
</pre>
<p>
The example above passes each record straight through to the feeder as defined by the output format class.
To feed, no reduce tasks are needed, for special requirements,
such as the need to guarantee a certain order of the operations for each document,
one can reduce on the document id and sort it out in the reducer.
</p>



<h2 id="configuration">Configuration</h2>
<p>
Parameters are sent using <code>-Dparam=value</code> for Pig and MapReduce.
For Oozie, specify them as parameters in the configuration section of the action.
</p>
<table class="table">
  <thead>
    <tr><th>parameter</th><th>default</th><th>description</th></tr>
  </thead><tbody>
    <tr><th>vespa.feed.endpoint</th>
      <td></td>
      <td><p id="vespa.feed.endpoint">Feed endpoint(s)</p></td></tr>
    <tr><th>vespa.feed.defaultport</th>
      <td></td>
      <td><p id="vespa.feed.defaultport">Feed endpoint port (e.g 8080)</p></td></tr>
    <tr><th>vespa.feed.proxy.host</th>
      <td></td>
      <td><p id="vespa.feed.proxy.host">HTTP proxy hostname</p></td></tr>
    <tr><th>vespa.feed.proxy.port</th>
      <td></td>
      <td><p id="vespa.feed.proxy.port">HTTP proxy port</p></td></tr>
    <tr><th>vespa.feed.dryrun</th>
      <td><em>false</em></td>
      <td><p id="vespa.feed.dryrun">Do not feed if set to <em>true</em></p></td></tr>
    <tr><th>vespa.feed.usecompression</th>
      <td><em>true</em>
      </td><td><p id="vespa.feed.usecompression">Compress the feed</p></td></tr>
    <tr><th>vespa.feed.data.format</th>
      <td><em>json</em></td>
      <td><p id="vespa.feed.data.format">The only supported format is <em>json</em>
        <!-- or "xml". Default is "json", "xml" is currently not supported.--></p></td></tr>
    <tr><th>vespa.feed.progress.interval</th>
      <td><em>1000</em></td>
      <td><p id="vespa.feed.progress.interval">Log sent/ok/failed/skipped documents every n records</p></td></tr>
    <tr><th>vespa.feed.connections</th>
      <td><em>1</em></td>
      <td><p id="vespa.feed.connections">Number of concurrent connections per mapper/reducer</p></td></tr>
    <tr><th>vespa.feed.route</th>
      <td><em>default</em></td>
      <td><p id="vespa.feed.route"><a href="routing.html">Route</a> to use on endpoint</p></td></tr>
  </tbody>
</table>
